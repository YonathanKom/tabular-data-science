{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a9f84a-a7aa-4841-b1f6-c2258f939876",
   "metadata": {},
   "source": [
    "# Comparative Analysis of Interestingness Measures in Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a350b-4bf8-487e-9e25-af2a1f45667f",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This notebook was developed with assistance from Claude. Claude helped with content generation, code suggestions, and structure organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddcd58-209b-4eb9-96b6-25d90587e190",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Data Loading](#setup)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Association Rule Mining](#mining)\n",
    "5. [Interestingness Measures Implementation](#measures)\n",
    "6. [Comparative Analysis](#analysis)\n",
    "7. [Cross-Dataset Stability Analysis](#cross-dataset)\n",
    "8. [Statistical Robustness Testing](#statistical)\n",
    "9. [Visualization of Results](#visualization)\n",
    "10. [Conclusions and Recommendations](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c38d6-2953-40ba-bf17-9c61dbfd8f41",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b1b0d-c379-48a4-84b9-bd7a81257229",
   "metadata": {},
   "source": [
    "This notebook implements the research proposal on improving the evaluation of association rules in pattern mining by comparing existing interestingness measures. We'll analyze five datasets to determine which measures work best for different data types and provide visual comparisons of their effectiveness.\n",
    "\n",
    "**Research Goals:**\n",
    "- Compare the effectiveness of existing interestingness measures\n",
    "- Determine which measures work best for different types of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7a1cd-14e5-4b23-8e15-848aaacdfe5b",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d1092-6e7f-4eb7-ac9a-cbdd39e26768",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries and loading our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66195fec-dd87-46f8-afac-5492196b62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from scipy.stats import spearmanr, wilcoxon\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "colors = sns.color_palette(\"viridis\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb006b74-075a-4542-a7a3-9b9ea1f7684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, dataset_type):\n",
    "    if dataset_type == 'adult':\n",
    "        columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                   'marital-status', 'occupation', 'relationship', 'race', \n",
    "                   'sex', 'capital-gain', 'capital-loss', 'hours-per-week', \n",
    "                   'native-country', 'income']\n",
    "        return pd.read_csv(filename, names=columns, sep=', ', engine='python')\n",
    "    \n",
    "    elif dataset_type == 'mushroom':\n",
    "        # Mushroom dataset has no header\n",
    "        return pd.read_csv(filename, header=None)\n",
    "    \n",
    "    elif dataset_type == 'bank':\n",
    "        return pd.read_csv(filename, sep=';')\n",
    "    \n",
    "    elif dataset_type == 'german':\n",
    "        # German credit data\n",
    "        columns = ['status', 'duration', 'credit_history', 'purpose', 'amount',\n",
    "                   'savings', 'employment_duration', 'installment_rate', 'personal_status_sex',\n",
    "                   'other_debtors', 'residence_since', 'property', 'age', 'other_installment_plans',\n",
    "                   'housing', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker',\n",
    "                   'credit_risk']\n",
    "        return pd.read_csv(filename, sep=' ', names=columns)\n",
    "    \n",
    "    elif dataset_type == 'house_prices':\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset type: {dataset_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e685285d-4fdf-4e8c-a345-2278c61af836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "try:\n",
    "    adult_df = load_dataset('datasets/adult.data', 'adult')\n",
    "    mushroom_df = load_dataset('datasets/agaricus-lepiota.data', 'mushroom')\n",
    "    bank_df = load_dataset('datasets/bank-full.csv', 'bank')\n",
    "    german_df = load_dataset('datasets/german.data', 'german')\n",
    "    house_df = load_dataset('datasets/house_prices.csv', 'house_prices')\n",
    "    \n",
    "    datasets = {\n",
    "        'Adult Census': adult_df,\n",
    "        'Mushroom': mushroom_df,\n",
    "        'Bank Marketing': bank_df,\n",
    "        'German Credit': german_df,\n",
    "        'House Prices': house_df\n",
    "    }\n",
    "    \n",
    "    print(\"All datasets loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a60379-ff8e-46be-9322-615755493dc6",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7298d9d-7de0-4aa5-8c5e-70f49eda12cd",
   "metadata": {},
   "source": [
    "We need to convert our datasets into a transaction format suitable for association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c74c6d6-2736-4959-8a60-d9f3d49e3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df, categorical_cols=None, numerical_cols=None, binary_cols=None, target_col=None):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        if df_copy[col].dtype == 'object':\n",
    "            df_copy[col] = df_copy[col].fillna('Unknown')\n",
    "        else:\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    df_transformed = pd.DataFrame()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            if col in df_copy.columns:\n",
    "                # One-hot encode categorical variables\n",
    "                df_transformed = pd.concat([\n",
    "                    df_transformed, \n",
    "                    pd.get_dummies(df_copy[col], prefix=col, drop_first=False)\n",
    "                ], axis=1)\n",
    "    \n",
    "    if numerical_cols:\n",
    "        for col in numerical_cols:\n",
    "            if col in df_copy.columns:\n",
    "                # Bin numerical variables into 5 categories\n",
    "                try:\n",
    "                    df_copy[f\"{col}_binned\"] = pd.qcut(\n",
    "                        df_copy[col], q=5, duplicates='drop', \n",
    "                        labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                    )\n",
    "                except:\n",
    "                    df_copy[f\"{col}_binned\"] = pd.cut(\n",
    "                        df_copy[col], bins=5, duplicates='drop', \n",
    "                        labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                    )\n",
    "                # One-hot encode binned variables\n",
    "                df_transformed = pd.concat([\n",
    "                    df_transformed, \n",
    "                    pd.get_dummies(df_copy[f\"{col}_binned\"], prefix=col, drop_first=False)\n",
    "                ], axis=1)\n",
    "    \n",
    "    if binary_cols:\n",
    "        for col in binary_cols:\n",
    "            if col in df_copy.columns:\n",
    "                df_transformed[col] = df_copy[col]\n",
    "    \n",
    "    if target_col and target_col in df_copy.columns:\n",
    "        if df_copy[target_col].dtype == 'object':\n",
    "            df_transformed = pd.concat([\n",
    "                df_transformed, \n",
    "                pd.get_dummies(df_copy[target_col], prefix=target_col, drop_first=False)\n",
    "            ], axis=1)\n",
    "        else:\n",
    "            try:\n",
    "                df_copy[f\"{target_col}_binned\"] = pd.qcut(\n",
    "                    df_copy[target_col], q=5, duplicates='drop', \n",
    "                    labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                )\n",
    "            except:\n",
    "                df_copy[f\"{target_col}_binned\"] = pd.cut(\n",
    "                    df_copy[target_col], bins=5, duplicates='drop', \n",
    "                    labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                )\n",
    "            df_transformed = pd.concat([\n",
    "                df_transformed, \n",
    "                pd.get_dummies(df_copy[f\"{target_col}_binned\"], prefix=target_col, drop_first=False)\n",
    "            ], axis=1)\n",
    "    \n",
    "    df_transformed = df_transformed.astype(int)\n",
    "    \n",
    "    transactions = df_transformed.apply(\n",
    "        lambda row: row.index[row == 1].tolist(),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "    \n",
    "    return df_transformed, transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9296c-6b7c-4aff-b0d5-ffb1d3585a4f",
   "metadata": {},
   "source": [
    "This function, `preprocess_dataset`, is designed to transform a given dataset by handling missing values, encoding categorical variables, binning numerical columns, and preparing the data for analysis. It also returns a transactional representation of the dataset.\n",
    "\n",
    "### **Processing Steps**\n",
    "1. **Handling Missing Values:**\n",
    "   - Categorical columns (`dtype=object`) are filled with `\"Unknown\"`.\n",
    "   - Numerical columns are filled with their median value.\n",
    "\n",
    "2. **Categorical Encoding:**\n",
    "   - Categorical columns specified in `categorical_cols` are one-hot encoded.\n",
    "\n",
    "3. **Numerical Binning:**\n",
    "   - Columns in `numerical_cols` are binned into 5 quantiles (`qcut` or `cut` as a fallback).\n",
    "   - Binned categories are one-hot encoded.\n",
    "\n",
    "4. **Binary Columns:**\n",
    "   - Binary columns are kept as is.\n",
    "\n",
    "5. **Target Column Transformation:**\n",
    "   - If categorical, one-hot encoding is applied.\n",
    "   - If numerical, it is binned into 5 quantiles (or using `cut` as a fallback) and one-hot encoded.\n",
    "\n",
    "6. **Final Transformation:**\n",
    "   - All encoded columns are converted to integers.\n",
    "   - A transaction-style dataset is generated where each row contains the names of active (nonzero) features.\n",
    "\n",
    "### **Function Output**\n",
    "- **df_transformed (DataFrame):** The processed dataset with numerical and categorical encoding.\n",
    "- **transactions (list of lists):** A transactional format where each row contains active feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f99299-d288-4696-a0b9-bfdba131ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_specs = {\n",
    "    'Adult Census': {\n",
    "        'categorical_cols': ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                            'relationship', 'race', 'sex', 'native-country'],\n",
    "        'numerical_cols': ['age', 'fnlwgt', 'education-num', 'capital-gain', \n",
    "                          'capital-loss', 'hours-per-week'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'income'\n",
    "    },\n",
    "    'Mushroom': {\n",
    "        'categorical_cols': list(range(1, 23)),\n",
    "        'numerical_cols': [],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 0\n",
    "    },\n",
    "    'Bank Marketing': {\n",
    "        'categorical_cols': ['job', 'marital', 'education', 'default', 'housing', \n",
    "                            'loan', 'contact', 'month', 'poutcome'],\n",
    "        'numerical_cols': ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'y'\n",
    "    },\n",
    "    'German Credit': {\n",
    "        'categorical_cols': ['status', 'credit_history', 'purpose', 'savings', \n",
    "                            'employment_duration', 'personal_status_sex', 'other_debtors',\n",
    "                            'property', 'other_installment_plans', 'housing', 'job',\n",
    "                            'telephone', 'foreign_worker'],\n",
    "        'numerical_cols': ['duration', 'amount', 'installment_rate', 'residence_since', \n",
    "                          'age', 'number_credits', 'people_liable'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'credit_risk'\n",
    "    },\n",
    "    'House Prices': {\n",
    "        'categorical_cols': ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', \n",
    "                            'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', \n",
    "                            'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \n",
    "                            'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \n",
    "                            'MasVnrType', 'Foundation', 'Heating', 'CentralAir', \n",
    "                            'Electrical', 'Functional', 'GarageType', 'PavedDrive', \n",
    "                            'Fence', 'MiscFeature', 'SaleType', 'SaleCondition'],\n",
    "        'numerical_cols': ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', \n",
    "                          'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', \n",
    "                          'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n",
    "                          '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \n",
    "                          'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n",
    "                          'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', \n",
    "                          'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n",
    "                          'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n",
    "                          'MiscVal', 'MoSold', 'YrSold'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'SalePrice'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df8d2ec-0ff6-44e3-ad5a-14db47e370bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14c06eb13f3493fb58b17a005461991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Adult Census dataset: 134 features\n",
      "Preprocessed Mushroom dataset: 117 features\n",
      "Preprocessed Bank Marketing dataset: 81 features\n",
      "Preprocessed German Credit dataset: 94 features\n",
      "Preprocessed House Prices dataset: 374 features\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = {}\n",
    "print(\"Preprocessing datasets...\")\n",
    "\n",
    "for name, df in tqdm(datasets.items()):\n",
    "    specs = preprocessing_specs.get(name, {\n",
    "        'categorical_cols': df.select_dtypes(include=['object']).columns.tolist(),\n",
    "        'numerical_cols': df.select_dtypes(include=['int64', 'float64']).columns.tolist(),\n",
    "        'binary_cols': [],\n",
    "        'target_col': None\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        df_transformed, transactions = preprocess_dataset(\n",
    "            df, \n",
    "            categorical_cols=specs['categorical_cols'],\n",
    "            numerical_cols=specs['numerical_cols'],\n",
    "            binary_cols=specs['binary_cols'],\n",
    "            target_col=specs['target_col']\n",
    "        )\n",
    "        \n",
    "        preprocessed_data[name] = {\n",
    "            'transformed_df': df_transformed,\n",
    "            'transactions': transactions,\n",
    "            'original_df': df\n",
    "        }\n",
    "        print(f\"Preprocessed {name} dataset: {df_transformed.shape[1]} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5f892-cf71-4a6c-a9d8-a2a979791757",
   "metadata": {},
   "source": [
    "## 4. Association Rule Mining <a id=\"mining\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d27eb2-93f8-448b-9542-a118e8c0833a",
   "metadata": {},
   "source": [
    "Now we'll perform association rule mining using the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9af994-9c4f-4ece-bbfe-44982e5abfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_association_rules(transactions, min_support=0.1, min_confidence=0.5, max_length=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_length, verbose=0)\n",
    "    \n",
    "    if not frequent_itemsets.empty:\n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        end_time = time.time()\n",
    "        print(f\"Mining completed in {end_time - start_time:.2f} seconds. Found {len(rules)} rules.\")\n",
    "        return frequent_itemsets, rules\n",
    "    else:\n",
    "        print(\"No frequent itemsets found with the current parameters.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fdefa5d-c0e6-4b98-9a6e-a0da5ac631fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_params = {\n",
    "    'Adult Census': {'min_support': 0.05, 'min_confidence': 0.5, 'max_length': 3},\n",
    "    'Mushroom': {'min_support': 0.2, 'min_confidence': 0.7, 'max_length': 3},\n",
    "    'Bank Marketing': {'min_support': 0.05, 'min_confidence': 0.5, 'max_length': 3},\n",
    "    'German Credit': {'min_support': 0.1, 'min_confidence': 0.6, 'max_length': 3},\n",
    "    'House Prices': {'min_support': 0.1, 'min_confidence': 0.5, 'max_length': 3}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df002a5-cdd1-437e-8fc5-24b3bbf05ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining association rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b09ed754a844be95f2dea8aaedccac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mining rules for Adult Census...\n",
      "Mining completed in 3.02 seconds. Found 5076 rules.\n",
      "Found 2433 frequent itemsets and 5076 rules\n",
      "\n",
      "Mining rules for Mushroom...\n",
      "Mining completed in 0.29 seconds. Found 3232 rules.\n",
      "Found 1594 frequent itemsets and 3232 rules\n",
      "\n",
      "Mining rules for Bank Marketing...\n",
      "Mining completed in 4.53 seconds. Found 7827 rules.\n",
      "Found 3669 frequent itemsets and 7827 rules\n",
      "\n",
      "Mining rules for German Credit...\n",
      "Mining completed in 0.09 seconds. Found 5089 rules.\n",
      "Found 3036 frequent itemsets and 5089 rules\n",
      "\n",
      "Mining rules for House Prices...\n",
      "Mining completed in 5.60 seconds. Found 353412 rules.\n",
      "Found 111472 frequent itemsets and 353412 rules\n"
     ]
    }
   ],
   "source": [
    "mining_results = {}\n",
    "print(\"Mining association rules...\")\n",
    "\n",
    "for name, data in tqdm(preprocessed_data.items()):\n",
    "    params = mining_params.get(name, {'min_support': 0.1, 'min_confidence': 0.5, 'max_length': 3})\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nMining rules for {name}...\")\n",
    "        frequent_itemsets, rules = mine_association_rules(\n",
    "            data['transactions'],\n",
    "            min_support=params['min_support'],\n",
    "            min_confidence=params['min_confidence'],\n",
    "            max_length=params['max_length']\n",
    "        )\n",
    "        \n",
    "        if not rules.empty:\n",
    "            mining_results[name] = {\n",
    "                'frequent_itemsets': frequent_itemsets,\n",
    "                'rules': rules\n",
    "            }\n",
    "            print(f\"Found {len(frequent_itemsets)} frequent itemsets and {len(rules)} rules\")\n",
    "        else:\n",
    "            print(f\"No rules found for {name} dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error mining rules for {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a9d14-3860-40b7-a02b-9af4cfe86faf",
   "metadata": {},
   "source": [
    "## 5. Interestingness Measures Implementation <a id=\"measures\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b402e2-b071-41f9-b0a1-08eef9f436bd",
   "metadata": {},
   "source": [
    "We'll implement a range of interestingness measures for evaluating association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f68915-4b5f-47a4-8f01-8d1d76ca0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_additional_measures(rules):\n",
    "    rules_copy = rules.copy()\n",
    "    \n",
    "    # Standard measures already in the dataframe:\n",
    "    # - support\n",
    "    # - confidence\n",
    "    # - lift\n",
    "    \n",
    "    # Calculate additional measures\n",
    "    \n",
    "    # Conviction: measure of implication strength\n",
    "    rules_copy['conviction'] = np.where(\n",
    "        (1 - rules_copy['confidence']) == 0, \n",
    "        float('inf'), \n",
    "        (1 - rules_copy['antecedent support']) / (1 - rules_copy['confidence'])\n",
    "    )\n",
    "    \n",
    "    # Leverage (Piatetsky-Shapiro): difference between observed and expected frequency\n",
    "    rules_copy['leverage'] = rules_copy['support'] - (rules_copy['antecedent support'] * rules_copy['consequent support'])\n",
    "    \n",
    "    # Jaccard coefficient: similarity measure\n",
    "    rules_copy['jaccard'] = rules_copy['support'] / (\n",
    "        rules_copy['antecedent support'] + rules_copy['consequent support'] - rules_copy['support']\n",
    "    )\n",
    "    \n",
    "    # Cosine: normalized measure of co-occurrence\n",
    "    rules_copy['cosine'] = rules_copy['support'] / np.sqrt(\n",
    "        rules_copy['antecedent support'] * rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    # Kulczynski: average of two conditional probabilities\n",
    "    rules_copy['kulczynski'] = 0.5 * (\n",
    "        rules_copy['confidence'] + rules_copy['support'] / rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    # All-confidence: minimum of the two confidence values\n",
    "    rules_copy['all_confidence'] = rules_copy['support'] / np.maximum(\n",
    "        rules_copy['antecedent support'], rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    # Collective strength\n",
    "    # P(violation in real data) / P(violation in independent case)\n",
    "    p_v_real = 1 - rules_copy['support'] - (\n",
    "        rules_copy['antecedent support'] * (1 - rules_copy['consequent support']) + \n",
    "        (1 - rules_copy['antecedent support']) * rules_copy['consequent support']\n",
    "    )\n",
    "    p_v_ind = 1 - (\n",
    "        rules_copy['antecedent support'] * rules_copy['consequent support'] + \n",
    "        (1 - rules_copy['antecedent support']) * (1 - rules_copy['consequent support'])\n",
    "    )\n",
    "    rules_copy['collective_strength'] = np.where(\n",
    "        p_v_ind == 0, \n",
    "        float('inf'), \n",
    "        (1 - p_v_real) / (1 - p_v_ind) * p_v_ind / p_v_real\n",
    "    )\n",
    "    \n",
    "    # Gini index\n",
    "    p_xy = rules_copy['support']\n",
    "    p_x = rules_copy['antecedent support']\n",
    "    p_y = rules_copy['consequent support']\n",
    "    p_not_x = 1 - p_x\n",
    "    p_not_y = 1 - p_y\n",
    "    p_x_y = p_xy / p_x  # P(Y|X)\n",
    "    p_x_not_y = (p_x - p_xy) / p_x  # P(¬Y|X)\n",
    "    p_not_x_y = (p_y - p_xy) / p_not_x  # P(Y|¬X)\n",
    "    p_not_x_not_y = (1 - p_x - p_y + p_xy) / p_not_x  # P(¬Y|¬X)\n",
    "    \n",
    "    gini_x = p_x * (p_x_y**2 + p_x_not_y**2) + p_not_x * (p_not_x_y**2 + p_not_x_not_y**2)\n",
    "    gini_y = p_y**2 + p_not_y**2\n",
    "    \n",
    "    rules_copy['gini_index'] = gini_x - gini_y\n",
    "    \n",
    "    # Piatetsky-Shapiro: deviation from independence\n",
    "    rules_copy['ps'] = rules_copy['support'] - (rules_copy['antecedent support'] * rules_copy['consequent support'])\n",
    "    \n",
    "    # Odds ratio: ratio of odds of occurrence\n",
    "    p_xy = rules_copy['support']\n",
    "    p_x = rules_copy['antecedent support']\n",
    "    p_y = rules_copy['consequent support']\n",
    "    p_not_xy = 1 - p_x - p_y + p_xy\n",
    "    rules_copy['odds_ratio'] = np.where(\n",
    "        (p_x - p_xy) * (p_y - p_xy) == 0, \n",
    "        float('inf'), \n",
    "        (p_xy * p_not_xy) / ((p_x - p_xy) * (p_y - p_xy))\n",
    "    )\n",
    "    \n",
    "    # Klosgen: combines support and confidence\n",
    "    rules_copy['klosgen'] = np.sqrt(rules_copy['support']) * (\n",
    "        rules_copy['confidence'] - rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    return rules_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3323cb-4547-409a-85ee-c5b7eda35937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating interestingness measures for Adult Census...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for Mushroom...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for Bank Marketing...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for German Credit...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for House Prices...\n",
      "Calculated 10 additional measures\n"
     ]
    }
   ],
   "source": [
    "for name, result in mining_results.items():\n",
    "    if 'rules' in result and not result['rules'].empty:\n",
    "        print(f\"Calculating interestingness measures for {name}...\")\n",
    "        try:\n",
    "            mining_results[name]['rules_with_measures'] = calculate_additional_measures(result['rules'])\n",
    "            print(f\"Calculated {len(mining_results[name]['rules_with_measures'].columns) - 9} additional measures\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating measures for {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5a654-6507-4f84-9881-19a3bf172ee0",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis <a id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ce575-9d7d-45ed-9986-41dccb08f4cf",
   "metadata": {},
   "source": [
    "Now, let's analyze the effectiveness of different interestingness measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e248609-6e11-4b75-b5af-4263a6f40626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_measure_correlations(rules_df):\n",
    "    measure_cols = [col for col in rules_df.columns if col not in [\n",
    "        'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "    ]]\n",
    "    \n",
    "    correlation_matrix = rules_df[measure_cols].corr(method='spearman')\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a45fc5b-c9f8-4981-9006-13cad2964b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rule_rankings(rules_df, top_n=50):\n",
    "    measure_cols = [col for col in rules_df.columns if col not in [\n",
    "        'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "    ]]\n",
    "    \n",
    "    top_rules = {}\n",
    "    for measure in measure_cols:\n",
    "        sorted_rules = rules_df.sort_values(by=measure, ascending=False)\n",
    "        top_rules[measure] = sorted_rules.head(top_n)\n",
    "    \n",
    "    jaccard_similarity = pd.DataFrame(index=measure_cols, columns=measure_cols, dtype=float)\n",
    "    for m1 in measure_cols:\n",
    "        for m2 in measure_cols:\n",
    "            top_m1 = set(top_rules[m1].index)\n",
    "            top_m2 = set(top_rules[m2].index)\n",
    "            intersection = len(top_m1.intersection(top_m2))\n",
    "            union = len(top_m1.union(top_m2))\n",
    "            jaccard_similarity.loc[m1, m2] = intersection / union if union > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'top_rules': top_rules,\n",
    "        'jaccard_similarity': jaccard_similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4900cf-869f-45eb-b376-d2e40fbacb89",
   "metadata": {},
   "source": [
    "The `analyze_rule_rankings` function processes a given DataFrame of association rules to identify the top-ranked rules based on various evaluation measures. It also computes the Jaccard similarity between the top-ranked rules across different measures.\n",
    "\n",
    "### **Processing Steps**\n",
    "1. **Identify Relevant Columns:**\n",
    "   - The function first identifies all columns in the `rules_df` except for the ones related to rule support and the antecedents/consequents (e.g., `'antecedent support'`, `'consequent support'`, `'antecedents'`, `'consequents'`).\n",
    "\n",
    "2. **Sorting and Ranking:**\n",
    "   - For each evaluation metric (measure), the rules are sorted in descending order based on that metric. The top `n` rules for each measure are stored in the `top_rules` dictionary.\n",
    "\n",
    "3. **Jaccard Similarity Calculation:**\n",
    "   - The function computes the Jaccard similarity between the top-ranked rules for each evaluation measure. This similarity is calculated by comparing the sets of top-ranked rule indices for each pair of measures. The formula for Jaccard similarity is:\n",
    "   \n",
    "   $$\n",
    "   \\text{Jaccard Similarity} = \\frac{\\text{Intersection of top rules}}{\\text{Union of top rules}}\n",
    "   $$\n",
    "   - A similarity matrix is created where each cell represents the Jaccard similarity between two measures.\n",
    "\n",
    "### **Function Output**\n",
    "The function returns a dictionary with two key elements:\n",
    "1. **top_rules (dict):** A dictionary where each key is a measure, and the value is a DataFrame containing the top `n` rules based on that measure.\n",
    "2. **jaccard_similarity (DataFrame):** A matrix containing the Jaccard similarity values between the top rules for each pair of measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd49b7c-dce8-4879-95ea-9544cf041d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rule_diversity(rules_df, measure, top_n=50):\n",
    "    sorted_rules = rules_df.sort_values(by=measure, ascending=False)\n",
    "    top_rules = sorted_rules.head(top_n)\n",
    "    \n",
    "    antecedent_items = set()\n",
    "    for items in top_rules['antecedents']:\n",
    "        antecedent_items.update(items)\n",
    "    \n",
    "    consequent_items = set()\n",
    "    for items in top_rules['consequents']:\n",
    "        consequent_items.update(items)\n",
    "    \n",
    "    support_range = (top_rules['support'].min(), top_rules['support'].max())\n",
    "    support_std = top_rules['support'].std()\n",
    "    \n",
    "    confidence_range = (top_rules['confidence'].min(), top_rules['confidence'].max())\n",
    "    confidence_std = top_rules['confidence'].std()\n",
    "    \n",
    "    # Calculate entropy-based diversity\n",
    "    # Higher entropy means more diverse rules\n",
    "    unique_antecedents = top_rules['antecedents'].apply(frozenset).value_counts()\n",
    "    p_antecedents = unique_antecedents / unique_antecedents.sum()\n",
    "    entropy_antecedents = -np.sum(p_antecedents * np.log2(p_antecedents))\n",
    "    \n",
    "    unique_consequents = top_rules['consequents'].apply(frozenset).value_counts()\n",
    "    p_consequents = unique_consequents / unique_consequents.sum()\n",
    "    entropy_consequents = -np.sum(p_consequents * np.log2(p_consequents))\n",
    "    \n",
    "    return {\n",
    "        'num_antecedent_items': len(antecedent_items),\n",
    "        'num_consequent_items': len(consequent_items),\n",
    "        'support_range': support_range,\n",
    "        'support_std': support_std,\n",
    "        'confidence_range': confidence_range,\n",
    "        'confidence_std': confidence_std,\n",
    "        'entropy_antecedents': entropy_antecedents,\n",
    "        'entropy_consequents': entropy_consequents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d00bcf-8571-447d-a391-29d197382b91",
   "metadata": {},
   "source": [
    "The `analyze_rule_diversity` function evaluates the diversity of association rules based on a specified evaluation measure. It computes various statistics related to the antecedents, consequents, and the overall rule characteristics, including entropy-based diversity metrics.\n",
    "\n",
    "### **Processing Steps**\n",
    "1. **Sorting and Ranking:**\n",
    "   - The function sorts the rules in descending order based on the specified evaluation measure (`measure`) and selects the top `n` rules.\n",
    "\n",
    "2. **Antecedent and Consequent Item Collection:**\n",
    "   - The antecedent and consequent items are extracted from the top-ranked rules and stored in sets, ensuring uniqueness.\n",
    "\n",
    "3. **Support and Confidence Statistics:**\n",
    "   - The function calculates the range and standard deviation of the `support` and `confidence` values for the top `n` rules.\n",
    "\n",
    "4. **Entropy-Based Diversity Calculation:**\n",
    "   - **Entropy** is used to measure the diversity of antecedents and consequents:\n",
    "     - **Higher entropy** indicates more diversity in the antecedent and consequent sets.\n",
    "     - For both antecedents and consequents, the function calculates the distribution of unique items and then computes entropy using the formula:\n",
    "       \n",
    "       $$\n",
    "       H(X) = -\\sum p(x) \\log_2(p(x))\n",
    "       $$\n",
    "       where $ p(x) $ is the probability of occurrence of a particular antecedent or consequent item.\n",
    "\n",
    "### **Function Output**\n",
    "The function returns a dictionary containing the following diversity statistics:\n",
    "- **num_antecedent_items:** The total number of unique antecedent items in the top `n` rules.\n",
    "- **num_consequent_items:** The total number of unique consequent items in the top `n` rules.\n",
    "- **support_range:** The range (min, max) of the support values for the top `n` rules.\n",
    "- **support_std:** The standard deviation of the support values for the top `n` rules.\n",
    "- **confidence_range:** The range (min, max) of the confidence values for the top `n` rules.\n",
    "- **confidence_std:** The standard deviation of the confidence values for the top `n` rules.\n",
    "- **entropy_antecedents:** The entropy of the antecedent items, indicating the diversity of antecedents.\n",
    "- **entropy_consequents:** The entropy of the consequent items, indicating the diversity of consequents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca7ce042-1d81-43dd-9aae-24df536521a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing measures for Adult Census dataset...\n",
      "Analysis completed for Adult Census dataset\n",
      "\n",
      "Analyzing measures for Mushroom dataset...\n",
      "Analysis completed for Mushroom dataset\n",
      "\n",
      "Analyzing measures for Bank Marketing dataset...\n",
      "Analysis completed for Bank Marketing dataset\n",
      "\n",
      "Analyzing measures for German Credit dataset...\n",
      "Analysis completed for German Credit dataset\n",
      "\n",
      "Analyzing measures for House Prices dataset...\n",
      "Analysis completed for House Prices dataset\n"
     ]
    }
   ],
   "source": [
    "analysis_results = {}\n",
    "for name, result in mining_results.items():\n",
    "    if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "        print(f\"\\nAnalyzing measures for {name} dataset...\")\n",
    "        \n",
    "        rules_df = result['rules_with_measures']\n",
    "        \n",
    "        try:\n",
    "            correlation_matrix = analyze_measure_correlations(rules_df)\n",
    "            \n",
    "            ranking_analysis = analyze_rule_rankings(rules_df, top_n=min(50, len(rules_df)))\n",
    "            \n",
    "            diversity_analysis = {}\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            \n",
    "            for measure in measure_cols:\n",
    "                diversity_analysis[measure] = analyze_rule_diversity(\n",
    "                    rules_df, measure, top_n=min(50, len(rules_df))\n",
    "                )\n",
    "            \n",
    "            analysis_results[name] = {\n",
    "                'correlation_matrix': correlation_matrix,\n",
    "                'ranking_analysis': ranking_analysis,\n",
    "                'diversity_analysis': diversity_analysis\n",
    "            }\n",
    "            \n",
    "            print(f\"Analysis completed for {name} dataset\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea03e3c-4422-4a8f-8b42-f5a1272297d4",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Stability Analysis <a id=\"cross-dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576be238-778c-4f74-a952-cefb698f4e09",
   "metadata": {},
   "source": [
    "Let's analyze the stability of measures across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9418d14a-b9b1-43be-bd33-ea95d23d9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_dataset_stability():\n",
    "    all_measures = []\n",
    "    for name, result in mining_results.items():\n",
    "        if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "            rules_df = result['rules_with_measures']\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            all_measures.extend(measure_cols)\n",
    "    all_measures = list(set(all_measures))\n",
    "    \n",
    "    stability_metrics = {\n",
    "        'correlation_variation': {measure: [] for measure in all_measures},\n",
    "        'diversity_variation': {measure: [] for measure in all_measures}\n",
    "    }\n",
    "    \n",
    "    for measure in all_measures:\n",
    "        correlations_across_datasets = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'correlation_matrix' in result:\n",
    "                corr_matrix = result['correlation_matrix']\n",
    "                if measure in corr_matrix.columns:\n",
    "                    correlations = corr_matrix[measure].drop(measure).values\n",
    "                    correlations_across_datasets.append(correlations)\n",
    "                    \n",
    "        if len(correlations_across_datasets) > 1:\n",
    "            corr_arrays = [np.array(c) for c in correlations_across_datasets if len(c) > 0]\n",
    "            \n",
    "            if len(corr_arrays) > 1 and all(len(c) == len(corr_arrays[0]) for c in corr_arrays):\n",
    "                stacked = np.vstack(corr_arrays)\n",
    "                mean_corr = np.mean(stacked, axis=0)\n",
    "                std_corr = np.std(stacked, axis=0)\n",
    "                cv_corr = np.where(np.abs(mean_corr) > 1e-10, std_corr / np.abs(mean_corr), 0)\n",
    "                stability_metrics['correlation_variation'][measure] = np.mean(cv_corr)\n",
    "\n",
    "    for measure in all_measures:\n",
    "        diversity_metrics = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'diversity_analysis' in result and measure in result['diversity_analysis']:\n",
    "                diversity = result['diversity_analysis'][measure]\n",
    "                metrics = {\n",
    "                    'entropy_antecedents': diversity.get('entropy_antecedents', 0),\n",
    "                    'entropy_consequents': diversity.get('entropy_consequents', 0),\n",
    "                    'support_std': diversity.get('support_std', 0)\n",
    "                }\n",
    "                diversity_metrics.append(metrics)\n",
    "        \n",
    "        if len(diversity_metrics) > 1:\n",
    "            entropy_ant = [d['entropy_antecedents'] for d in diversity_metrics]\n",
    "            entropy_cons = [d['entropy_consequents'] for d in diversity_metrics]\n",
    "            support_std = [d['support_std'] for d in diversity_metrics]\n",
    "            \n",
    "            cv_entropy_ant = np.std(entropy_ant) / np.mean(entropy_ant) if np.mean(entropy_ant) > 0 else 0\n",
    "            cv_entropy_cons = np.std(entropy_cons) / np.mean(entropy_cons) if np.mean(entropy_cons) > 0 else 0\n",
    "            cv_support_std = np.std(support_std) / np.mean(support_std) if np.mean(support_std) > 0 else 0\n",
    "            \n",
    "            stability_metrics['diversity_variation'][measure] = np.mean([\n",
    "                cv_entropy_ant, cv_entropy_cons, cv_support_std\n",
    "            ])\n",
    "    \n",
    "    return stability_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69539fc8-a52c-4595-9640-48900847556c",
   "metadata": {},
   "source": [
    "The `analyze_cross_dataset_stability` function is designed to evaluate the stability of association rule mining results across different datasets. Specifically, it measures two key aspects of stability:\n",
    "1. **Correlation Variation**: The variation of correlation between different measures across datasets.\n",
    "2. **Diversity Variation**: The variation in diversity metrics (entropy of antecedents, entropy of consequents, and standard deviation of support) across datasets.\n",
    "\n",
    "### **Processing Steps**\n",
    "1. **Identify All Measures**:\n",
    "   - The function first scans through the `mining_results` and extracts all the measure columns from the association rules in `rules_with_measures`. These measures exclude specific columns like 'antecedent support', 'consequent support', 'antecedents', and 'consequents'.\n",
    "   - A unique set of measures across all results is stored for later use.\n",
    "\n",
    "2. **Correlation Variation Calculation**:\n",
    "   - For each measure, the function collects the correlation values from the `correlation_matrix` in the `analysis_results` for each dataset.\n",
    "   - It checks whether the correlation values for the same measure are consistent across multiple datasets.\n",
    "   - The **coefficient of variation (CV)** of the correlation is computed using the formula:\n",
    "     $$\n",
    "     CV_{\\text{corr}} = \\frac{\\text{std}(r)}{\\text{mean}(r)}\n",
    "     $$\n",
    "     where $ r $ represents the correlation values.\n",
    "   - The average CV for each measure is calculated to represent the stability of the correlation across datasets.\n",
    "\n",
    "3. **Diversity Variation Calculation**:\n",
    "   - The function also calculates the variation in diversity for each measure using the `diversity_analysis` from the `analysis_results`.\n",
    "   - It collects three diversity metrics: \n",
    "     - **Entropy of Antecedents**: Measures the diversity in the antecedents of the rules.\n",
    "     - **Entropy of Consequents**: Measures the diversity in the consequents of the rules.\n",
    "     - **Support Standard Deviation**: Measures the variation in the support of the rules.\n",
    "   - The CV for each diversity metric is calculated using the formula:\n",
    "     $$\n",
    "     CV_{\\text{diversity}} = \\frac{\\text{std}(d)}{\\text{mean}(d)}\n",
    "     $$\n",
    "     where $ d $ represents the respective diversity metrics (entropy or support).\n",
    "   - The average of the CVs for the antecedents, consequents, and support is used to represent the diversity variation for each measure.\n",
    "\n",
    "### **Function Output**\n",
    "The function returns a dictionary `stability_metrics` containing the following:\n",
    "- **correlation_variation**: A dictionary where each measure maps to its correlation variation (CV) across datasets.\n",
    "- **diversity_variation**: A dictionary where each measure maps to its diversity variation (CV) across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aa893cc-9919-437c-b867-139b471b560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-dataset stability analysis...\n",
      "Cross-dataset stability analysis completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing cross-dataset stability analysis...\")\n",
    "stability_metrics = analyze_cross_dataset_stability()\n",
    "\n",
    "stability_scores = {}\n",
    "for measure in stability_metrics['correlation_variation']:\n",
    "    if measure in stability_metrics['correlation_variation'] and measure in stability_metrics['diversity_variation']:\n",
    "        corr_var = stability_metrics['correlation_variation'][measure]\n",
    "        div_var = stability_metrics['diversity_variation'][measure]\n",
    "        \n",
    "        if not np.isnan(corr_var) and not np.isnan(div_var):\n",
    "            stability_scores[measure] = 1 - np.mean([\n",
    "                corr_var / max(stability_metrics['correlation_variation'].values()),\n",
    "                div_var / max(stability_metrics['diversity_variation'].values())\n",
    "            ])\n",
    "\n",
    "print(\"Cross-dataset stability analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ae299-9d9e-49d4-a4f2-9003ed0a7293",
   "metadata": {},
   "source": [
    "## 8. Statistical Robustness Testing <a id=\"statistical\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1e868-de35-4ff6-9044-a57efc09f5f6",
   "metadata": {},
   "source": [
    "Let's perform statistical tests to evaluate the robustness of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "676d1adf-388d-408f-9a68-f03dbae1757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_wilcoxon_tests(top_n=50):\n",
    "    all_measures = []\n",
    "    for name, result in mining_results.items():\n",
    "        if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "            rules_df = result['rules_with_measures']\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            all_measures.extend(measure_cols)\n",
    "    all_measures = list(set(all_measures))\n",
    "    \n",
    "    test_results = {\n",
    "        dataset_name: {\n",
    "            'p_values': pd.DataFrame(index=all_measures, columns=all_measures),\n",
    "            'significant_differences': pd.DataFrame(index=all_measures, columns=all_measures)\n",
    "        }\n",
    "        for dataset_name in mining_results.keys()\n",
    "    }\n",
    "    \n",
    "    for dataset_name, result in mining_results.items():\n",
    "        if 'rules_with_measures' not in result or result['rules_with_measures'].empty:\n",
    "            continue\n",
    "        \n",
    "        rules_df = result['rules_with_measures']\n",
    "        dataset_measures = [col for col in rules_df.columns if col not in [\n",
    "            'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "        ]]\n",
    "        \n",
    "        ranks = pd.DataFrame(index=rules_df.index)\n",
    "        for measure in dataset_measures:\n",
    "            ranks[measure] = rules_df[measure].rank(ascending=False)\n",
    "        \n",
    "        for m1 in dataset_measures:\n",
    "            for m2 in dataset_measures:\n",
    "                if m1 != m2:\n",
    "                    try:\n",
    "                        stat, p_value = wilcoxon(ranks[m1], ranks[m2])\n",
    "                        \n",
    "                        test_results[dataset_name]['p_values'].loc[m1, m2] = p_value\n",
    "                        \n",
    "                        test_results[dataset_name]['significant_differences'].loc[m1, m2] = p_value < 0.05\n",
    "                    except Exception:\n",
    "                        test_results[dataset_name]['p_values'].loc[m1, m2] = 1.0\n",
    "                        test_results[dataset_name]['significant_differences'].loc[m1, m2] = False\n",
    "                else:\n",
    "                    test_results[dataset_name]['p_values'].loc[m1, m2] = 1.0\n",
    "                    test_results[dataset_name]['significant_differences'].loc[m1, m2] = False\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de9df9-5343-4489-b6ff-b08397c21ab8",
   "metadata": {},
   "source": [
    "The `perform_wilcoxon_tests` function applies the **Wilcoxon Signed-Rank Test** to compare the rankings of various measures across multiple datasets. It evaluates whether the distributions of two measures are significantly different by comparing their rankings in each dataset.\n",
    "\n",
    "### **Processing Steps**\n",
    "1. **Identify Measures**:\n",
    "   - The function identifies all the relevant measures (columns) from the `rules_with_measures` DataFrame, excluding columns related to support and antecedents/consequents.\n",
    "\n",
    "2. **Rank Rules**:\n",
    "   - For each dataset in the `mining_results`, the rules are ranked based on their measures (excluding antecedent and consequent columns).\n",
    "   - The `rank` method ranks each rule from highest to lowest for each measure.\n",
    "\n",
    "3. **Wilcoxon Signed-Rank Test**:\n",
    "   - For each pair of measures $ m_1 $ and $ m_2 $, the function performs the **Wilcoxon Signed-Rank Test** to compare the ranks of these two measures.\n",
    "   - The Wilcoxon test assesses whether there is a significant difference in the distributions of the ranks of the two measures across the rules.\n",
    "   - The result is a p-value that indicates whether the difference is statistically significant (p-value < 0.05).\n",
    "\n",
    "4. **Test Results Storage**:\n",
    "   - The results are stored in a dictionary `test_results`, which contains the following data:\n",
    "     - **p_values**: A DataFrame of p-values for each pair of measures.\n",
    "     - **significant_differences**: A DataFrame of boolean values indicating whether the difference is statistically significant (True if p-value < 0.05).\n",
    "\n",
    "5. **Edge Cases**:\n",
    "   - If an exception occurs during the Wilcoxon test (for example, when the data for a particular measure cannot be compared), the p-value is set to 1.0, and the significant difference is marked as False.\n",
    "   - If two measures are the same, the p-value is set to 1.0 (no significant difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab6e9bfa-917a-491c-ab2c-436cd40ab39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing statistical robustness testing...\n",
      "Statistical testing completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing statistical robustness testing...\")\n",
    "statistical_tests = perform_wilcoxon_tests()\n",
    "print(\"Statistical testing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9c469-49af-4276-9844-cf5e4f5475b2",
   "metadata": {},
   "source": [
    "## 9. Visualization of Results <a id=\"visualization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b1034-bbd6-466c-ad96-203e856f0f7e",
   "metadata": {},
   "source": [
    "Now, let's visualize our results to better understand the relationships and effectiveness of different measures.\n",
    "**All the visualizations are saved in the visualizations folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c89150f-fbb8-4a0e-bf07-716778be8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_measure_correlations(dataset_name, correlation_matrix):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        correlation_matrix, \n",
    "        mask=mask,\n",
    "        annot=True, \n",
    "        fmt=\".2f\", \n",
    "        cmap='coolwarm', \n",
    "        vmin=-1, \n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Spearman Rank Correlation Between Measures ({dataset_name})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef067566-fd25-4d5e-a139-eb5d746d48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_measure_clusters(dataset_name, correlation_matrix):\n",
    "    distance_matrix = 1 - np.abs(correlation_matrix)\n",
    "    \n",
    "    linkage_matrix = scipy.cluster.hierarchy.linkage(\n",
    "        scipy.spatial.distance.squareform(distance_matrix), \n",
    "        method='average'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    scipy.cluster.hierarchy.dendrogram(\n",
    "        linkage_matrix,\n",
    "        labels=correlation_matrix.columns,\n",
    "        leaf_font_size=12,\n",
    "        color_threshold=0.5,\n",
    "        orientation='top'\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Hierarchical Clustering of Measures ({dataset_name})', fontsize=16)\n",
    "    plt.xlabel('Measures', fontsize=14)\n",
    "    plt.ylabel('Distance (1 - |correlation|)', fontsize=14)\n",
    "    plt.axhline(y=0.5, c='k', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d73bff54-f5a5-4bed-b057-343b9dea914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rule_diversity(dataset_name, diversity_analysis):\n",
    "    measures = list(diversity_analysis.keys())\n",
    "    entropy_ant = [diversity_analysis[m]['entropy_antecedents'] for m in measures]\n",
    "    entropy_cons = [diversity_analysis[m]['entropy_consequents'] for m in measures]\n",
    "    support_std = [diversity_analysis[m]['support_std'] for m in measures]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 16))\n",
    "    \n",
    "    axes[0].bar(range(len(measures)), entropy_ant, color=colors[:len(measures)])\n",
    "    axes[0].set_title('Entropy of Antecedents in Top Rules', fontsize=14)\n",
    "    axes[0].set_xticks(range(len(measures)))\n",
    "    axes[0].set_xticklabels(measures, rotation=90)\n",
    "    axes[0].set_ylabel('Entropy')\n",
    "    \n",
    "    axes[1].bar(range(len(measures)), entropy_cons, color=colors[:len(measures)])\n",
    "    axes[1].set_title('Entropy of Consequents in Top Rules', fontsize=14)\n",
    "    axes[1].set_xticks(range(len(measures)))\n",
    "    axes[1].set_xticklabels(measures, rotation=90)\n",
    "    axes[1].set_ylabel('Entropy')\n",
    "    \n",
    "    axes[2].bar(range(len(measures)), support_std, color=colors[:len(measures)])\n",
    "    axes[2].set_title('Standard Deviation of Support in Top Rules', fontsize=14)\n",
    "    axes[2].set_xticks(range(len(measures)))\n",
    "    axes[2].set_xticklabels(measures, rotation=90)\n",
    "    axes[2].set_ylabel('Standard Deviation')\n",
    "    \n",
    "    plt.suptitle(f'Rule Diversity Metrics ({dataset_name})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e67fa0db-2916-4987-bc7a-ee620fc20969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_jaccard_similarity(dataset_name, jaccard_similarity):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(jaccard_similarity, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        jaccard_similarity, \n",
    "        mask=mask,\n",
    "        annot=True, \n",
    "        fmt=\".2f\", \n",
    "        cmap='YlGnBu', \n",
    "        vmin=0, \n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "        xticklabels=jaccard_similarity.columns,\n",
    "        yticklabels=jaccard_similarity.index\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Jaccard Similarity Between Top Rules ({dataset_name})', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da194008-3a9a-4871-bee4-cff1a0fa0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cross_dataset_stability(stability_scores):\n",
    "    sorted_measures = sorted(stability_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    measures = [m[0] for m in sorted_measures]\n",
    "    scores = [m[1] for m in sorted_measures]\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(range(len(measures)), scores, color=colors[:len(measures)])\n",
    "    plt.xticks(range(len(measures)), measures, rotation=90)\n",
    "    plt.title('Cross-Dataset Stability of Interestingness Measures', fontsize=16)\n",
    "    plt.ylabel('Stability Score (higher is better)', fontsize=14)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240238dd-b0a3-4c99-a2ef-6c361d06c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_significance_matrix(dataset_name, significant_differences):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        significant_differences.astype(int), \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap='Reds', \n",
    "        vmin=0, \n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Significant Differences Between Measures ({dataset_name})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6924c965-336f-429e-be48-9d9104a1b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_measure_comparison_summary():\n",
    "    all_measures = []\n",
    "    for name, result in mining_results.items():\n",
    "        if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "            rules_df = result['rules_with_measures']\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            all_measures.extend(measure_cols)\n",
    "    all_measures = list(set(all_measures))\n",
    "    \n",
    "    summary = pd.DataFrame(index=all_measures, columns=[\n",
    "        'Avg Correlation', 'Stability Score', 'Avg Diversity', \n",
    "        'Significant Differences', 'Recommended For'\n",
    "    ])\n",
    "    \n",
    "    for measure in all_measures:\n",
    "        avg_corr = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'correlation_matrix' in result:\n",
    "                corr_matrix = result['correlation_matrix']\n",
    "                if measure in corr_matrix.columns:\n",
    "                    correlations = corr_matrix[measure].drop(measure).abs()\n",
    "                    avg_corr.append(correlations.mean())\n",
    "        \n",
    "        summary.loc[measure, 'Avg Correlation'] = np.mean(avg_corr) if avg_corr else np.nan\n",
    "        \n",
    "        summary.loc[measure, 'Stability Score'] = stability_scores.get(measure, np.nan)\n",
    "        \n",
    "        avg_div = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'diversity_analysis' in result and measure in result['diversity_analysis']:\n",
    "                diversity = result['diversity_analysis'][measure]\n",
    "                avg_div.append(diversity.get('entropy_antecedents', 0) + diversity.get('entropy_consequents', 0))\n",
    "        \n",
    "        summary.loc[measure, 'Avg Diversity'] = np.mean(avg_div) if avg_div else np.nan\n",
    "        \n",
    "        sig_diff_count = 0\n",
    "        total_tests = 0\n",
    "        for dataset_name, tests in statistical_tests.items():\n",
    "            if 'significant_differences' in tests:\n",
    "                sig_diff_matrix = tests['significant_differences']\n",
    "                if measure in sig_diff_matrix.index:\n",
    "                    sig_diff_count += sig_diff_matrix.loc[measure].sum()\n",
    "                    total_tests += len(sig_diff_matrix.columns) - 1\n",
    "        \n",
    "        summary.loc[measure, 'Significant Differences'] = sig_diff_count / total_tests if total_tests > 0 else np.nan\n",
    "    \n",
    "    for measure in all_measures:\n",
    "        recommendations = []\n",
    "        \n",
    "        stability = summary.loc[measure, 'Stability Score']\n",
    "        if not np.isnan(stability):\n",
    "            if stability > 0.8:\n",
    "                recommendations.append(\"Cross-Dataset Analysis\")\n",
    "            elif stability < 0.4:\n",
    "                recommendations.append(\"Dataset-Specific Analysis\")\n",
    "        \n",
    "        diversity = summary.loc[measure, 'Avg Diversity']\n",
    "        if not np.isnan(diversity):\n",
    "            if diversity > 1.5:\n",
    "                recommendations.append(\"Discovering Diverse Rules\")\n",
    "            elif diversity < 0.8:\n",
    "                recommendations.append(\"Finding Core Patterns\")\n",
    "        \n",
    "        correlation = summary.loc[measure, 'Avg Correlation']\n",
    "        if not np.isnan(correlation):\n",
    "            if correlation < 0.3:\n",
    "                recommendations.append(\"Unique Perspective\")\n",
    "            elif correlation > 0.7:\n",
    "                recommendations.append(\"Consensus Measure\")\n",
    "        \n",
    "        summary.loc[measure, 'Recommended For'] = \", \".join(recommendations)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a599a77-7bb9-4cc4-a3c7-39440705dd8d",
   "metadata": {},
   "source": [
    "The `create_measure_comparison_summary` function generates a summary table that compares various measures used in association rule mining across multiple datasets. The summary includes key metrics like average correlation, stability score, average diversity, significant differences, and recommendations for the usage of each measure.\n",
    "\n",
    "### **Processing Steps**\n",
    "1. **Identify All Measures**:\n",
    "   - The function first identifies all unique measures across all datasets. These measures are derived from the `rules_with_measures` DataFrame, excluding columns related to antecedent and consequent support.\n",
    "\n",
    "2. **Create Summary DataFrame**:\n",
    "   - A DataFrame `summary` is initialized with the measures as the index and the following columns:\n",
    "     - **Avg Correlation**: The average correlation of the measure across datasets.\n",
    "     - **Stability Score**: The stability score for the measure, retrieved from pre-calculated `stability_scores`.\n",
    "     - **Avg Diversity**: The average diversity of the measure across datasets, calculated from the entropy of antecedents and consequents.\n",
    "     - **Significant Differences**: The proportion of statistically significant differences found for the measure across datasets.\n",
    "     - **Recommended For**: A recommendation for the type of analysis based on the measure’s characteristics.\n",
    "\n",
    "3. **Compute Average Correlation**:\n",
    "   - For each measure, the function calculates the average correlation with other measures across different datasets. The correlation is taken from the `correlation_matrix` in `analysis_results`, excluding self-correlations (correlations of the measure with itself).\n",
    "   \n",
    "4. **Retrieve Stability Scores**:\n",
    "   - The function looks up the stability score for each measure from the `stability_scores` dictionary. If no score is found, `NaN` is used.\n",
    "\n",
    "5. **Calculate Average Diversity**:\n",
    "   - For each measure, the function calculates the average diversity across datasets. Diversity is based on the entropy of antecedents and consequents, which is stored in the `diversity_analysis` dictionary in `analysis_results`.\n",
    "\n",
    "6. **Calculate Significant Differences**:\n",
    "   - The function counts the number of significant differences for each measure, as identified by the Wilcoxon test. This is derived from the `significant_differences` matrix in `statistical_tests`.\n",
    "\n",
    "7. **Recommendations**:\n",
    "   - For each measure, the function generates recommendations based on the calculated metrics:\n",
    "     - **Stability**:\n",
    "       - If the stability score is high (> 0.8), recommend \"Cross-Dataset Analysis\".\n",
    "       - If the stability score is low (< 0.4), recommend \"Dataset-Specific Analysis\".\n",
    "     - **Diversity**:\n",
    "       - If the diversity is high (> 1.5), recommend \"Discovering Diverse Rules\".\n",
    "       - If the diversity is low (< 0.8), recommend \"Finding Core Patterns\".\n",
    "     - **Correlation**:\n",
    "       - If the average correlation is low (< 0.3), recommend \"Unique Perspective\".\n",
    "       - If the average correlation is high (> 0.7), recommend \"Consensus Measure\".\n",
    "   - The recommendations are stored as a string in the `Recommended For` column of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed4838e0-8f77-45d2-98b4-b0bd50a35ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualizations...\n",
      "Visualizing results for Adult Census dataset...\n",
      "Visualizing results for Mushroom dataset...\n",
      "Visualizing results for Bank Marketing dataset...\n",
      "Visualizing results for German Credit dataset...\n",
      "Visualizing results for House Prices dataset...\n",
      "Visualizations completed\n",
      "\n",
      "Summary Comparison of Interestingness Measures:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg Correlation</th>\n",
       "      <th>Stability Score</th>\n",
       "      <th>Avg Diversity</th>\n",
       "      <th>Significant Differences</th>\n",
       "      <th>Recommended For</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>odds_ratio</th>\n",
       "      <td>0.462323</td>\n",
       "      <td>0.658296</td>\n",
       "      <td>7.634986</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>0.292543</td>\n",
       "      <td>0.597289</td>\n",
       "      <td>6.868625</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules, Unique Perspective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gini_index</th>\n",
       "      <td>0.415608</td>\n",
       "      <td>0.327322</td>\n",
       "      <td>8.374914</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zhangs_metric</th>\n",
       "      <td>0.504369</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>7.670779</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_confidence</th>\n",
       "      <td>0.453595</td>\n",
       "      <td>0.654175</td>\n",
       "      <td>7.695466</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps</th>\n",
       "      <td>0.508589</td>\n",
       "      <td>0.530641</td>\n",
       "      <td>8.28845</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klosgen</th>\n",
       "      <td>0.499883</td>\n",
       "      <td>0.433946</td>\n",
       "      <td>8.057307</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine</th>\n",
       "      <td>0.411817</td>\n",
       "      <td>0.477737</td>\n",
       "      <td>7.876869</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lift</th>\n",
       "      <td>0.490595</td>\n",
       "      <td>0.501859</td>\n",
       "      <td>8.040964</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conviction</th>\n",
       "      <td>0.277443</td>\n",
       "      <td>0.23392</td>\n",
       "      <td>6.861409</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kulczynski</th>\n",
       "      <td>0.364443</td>\n",
       "      <td>0.31267</td>\n",
       "      <td>7.870534</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence</th>\n",
       "      <td>0.271603</td>\n",
       "      <td>0.182452</td>\n",
       "      <td>6.7178</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leverage</th>\n",
       "      <td>0.508589</td>\n",
       "      <td>0.530641</td>\n",
       "      <td>8.28845</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard</th>\n",
       "      <td>0.444167</td>\n",
       "      <td>0.658928</td>\n",
       "      <td>7.805386</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collective_strength</th>\n",
       "      <td>0.414743</td>\n",
       "      <td>0.344112</td>\n",
       "      <td>6.205637</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Avg Correlation Stability Score Avg Diversity  \\\n",
       "odds_ratio                 0.462323        0.658296      7.634986   \n",
       "support                    0.292543        0.597289      6.868625   \n",
       "gini_index                 0.415608        0.327322      8.374914   \n",
       "zhangs_metric              0.504369        0.501875      7.670779   \n",
       "all_confidence             0.453595        0.654175      7.695466   \n",
       "ps                         0.508589        0.530641       8.28845   \n",
       "klosgen                    0.499883        0.433946      8.057307   \n",
       "cosine                     0.411817        0.477737      7.876869   \n",
       "lift                       0.490595        0.501859      8.040964   \n",
       "conviction                 0.277443         0.23392      6.861409   \n",
       "kulczynski                 0.364443         0.31267      7.870534   \n",
       "confidence                 0.271603        0.182452        6.7178   \n",
       "leverage                   0.508589        0.530641       8.28845   \n",
       "jaccard                    0.444167        0.658928      7.805386   \n",
       "collective_strength        0.414743        0.344112      6.205637   \n",
       "\n",
       "                    Significant Differences  \\\n",
       "odds_ratio                         0.071429   \n",
       "support                            0.071429   \n",
       "gini_index                         0.057143   \n",
       "zhangs_metric                      0.071429   \n",
       "all_confidence                     0.071429   \n",
       "ps                                 0.071429   \n",
       "klosgen                            0.071429   \n",
       "cosine                             0.071429   \n",
       "lift                               0.071429   \n",
       "conviction                         0.071429   \n",
       "kulczynski                         0.071429   \n",
       "confidence                         0.071429   \n",
       "leverage                           0.071429   \n",
       "jaccard                            0.071429   \n",
       "collective_strength                0.071429   \n",
       "\n",
       "                                                       Recommended For  \n",
       "odds_ratio                                   Discovering Diverse Rules  \n",
       "support                  Discovering Diverse Rules, Unique Perspective  \n",
       "gini_index           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "zhangs_metric                                Discovering Diverse Rules  \n",
       "all_confidence                               Discovering Diverse Rules  \n",
       "ps                                           Discovering Diverse Rules  \n",
       "klosgen                                      Discovering Diverse Rules  \n",
       "cosine                                       Discovering Diverse Rules  \n",
       "lift                                         Discovering Diverse Rules  \n",
       "conviction           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "kulczynski           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "confidence           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "leverage                                     Discovering Diverse Rules  \n",
       "jaccard                                      Discovering Diverse Rules  \n",
       "collective_strength  Dataset-Specific Analysis, Discovering Diverse...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating visualizations...\")\n",
    "\n",
    "if not os.path.exists('visualizations'):\n",
    "    os.makedirs('visualizations')\n",
    "\n",
    "for name, result in analysis_results.items():\n",
    "    if 'correlation_matrix' in result:\n",
    "        print(f\"Visualizing results for {name} dataset...\")\n",
    "        \n",
    "        fig_corr = visualize_measure_correlations(name, result['correlation_matrix'])\n",
    "        fig_corr.savefig(f'visualizations/{name}_correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        try:\n",
    "            fig_cluster = visualize_measure_clusters(name, result['correlation_matrix'])\n",
    "            fig_cluster.savefig(f'visualizations/{name}_measure_clusters.png', dpi=150, bbox_inches='tight')\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating measure clusters for {name}: {e}\")\n",
    "        \n",
    "        if 'ranking_analysis' in result and 'jaccard_similarity' in result['ranking_analysis']:\n",
    "            jaccard_similarity = result['ranking_analysis']['jaccard_similarity']\n",
    "            fig_jaccard = visualize_jaccard_similarity(name, jaccard_similarity)\n",
    "            fig_jaccard.savefig(f'visualizations/{name}_jaccard_similarity.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        if 'diversity_analysis' in result:\n",
    "            fig_diversity = visualize_rule_diversity(name, result['diversity_analysis'])\n",
    "            fig_diversity.savefig(f'visualizations/{name}_rule_diversity.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        if name in statistical_tests and 'significant_differences' in statistical_tests[name]:\n",
    "            fig_sig = visualize_significance_matrix(name, statistical_tests[name]['significant_differences'])\n",
    "            fig_sig.savefig(f'visualizations/{name}_significant_differences.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        plt.close('all')\n",
    "\n",
    "if stability_scores:\n",
    "    fig_stability = visualize_cross_dataset_stability(stability_scores)\n",
    "    fig_stability.savefig('visualizations/cross_dataset_stability.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "\n",
    "summary_comparison = create_measure_comparison_summary()\n",
    "print(\"Visualizations completed\")\n",
    "\n",
    "print(\"\\nSummary Comparison of Interestingness Measures:\")\n",
    "display(summary_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac8354-9e96-467b-a435-262b25fa6f3a",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Recommendations <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4edf74-e96b-4e8d-9b28-cc374873cb78",
   "metadata": {},
   "source": [
    "Let's wrap up with conclusions and recommendations based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf3e7ead-0c4d-4b99-ba56-fc43a679e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations():\n",
    "    summary = create_measure_comparison_summary()\n",
    "    \n",
    "    top_stability = summary.sort_values(by='Stability Score', ascending=False).head(3).index.tolist()\n",
    "    top_diversity = summary.sort_values(by='Avg Diversity', ascending=False).head(3).index.tolist()\n",
    "    top_uniqueness = summary.sort_values(by='Avg Correlation', ascending=True).head(3).index.tolist()\n",
    "    \n",
    "    dataset_recommendations = {}\n",
    "    for name in mining_results.keys():\n",
    "        if name in analysis_results:\n",
    "            if 'rules_with_measures' in mining_results[name]:\n",
    "                rules_df = mining_results[name]['rules_with_measures']\n",
    "                \n",
    "                top_consensus_rules = []\n",
    "                if not rules_df.empty:\n",
    "                    measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                        'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "                    ]]\n",
    "                    ranks = pd.DataFrame(index=rules_df.index)\n",
    "                    for measure in measure_cols:\n",
    "                        ranks[measure] = rules_df[measure].rank(ascending=False)\n",
    "                    \n",
    "                    ranks['avg_rank'] = ranks.mean(axis=1)\n",
    "                    \n",
    "                    top_rules = ranks.sort_values(by='avg_rank').head(5)\n",
    "                    \n",
    "                    for idx in top_rules.index:\n",
    "                        antecedents = rules_df.loc[idx, 'antecedents']\n",
    "                        consequents = rules_df.loc[idx, 'consequents']\n",
    "                        support = rules_df.loc[idx, 'support']\n",
    "                        confidence = rules_df.loc[idx, 'confidence']\n",
    "                        \n",
    "                        rule_str = f\"{set(antecedents)} => {set(consequents)} [support={support:.3f}, confidence={confidence:.3f}]\"\n",
    "                        top_consensus_rules.append(rule_str)\n",
    "                \n",
    "                if 'correlation_matrix' in analysis_results[name]:\n",
    "                    corr_matrix = analysis_results[name]['correlation_matrix']\n",
    "                    \n",
    "                    avg_corr = {}\n",
    "                    for measure in corr_matrix.columns:\n",
    "                        avg_corr[measure] = corr_matrix[measure].drop(measure).abs().mean()\n",
    "                    \n",
    "                    unique_measures = sorted(avg_corr.items(), key=lambda x: x[1])[:3]\n",
    "                    unique_measures = [m[0] for m in unique_measures]\n",
    "                    \n",
    "                    diverse_measures = []\n",
    "                    if 'diversity_analysis' in analysis_results[name]:\n",
    "                        diversity_values = {}\n",
    "                        for measure, metrics in analysis_results[name]['diversity_analysis'].items():\n",
    "                            diversity_values[measure] = metrics.get('entropy_antecedents', 0) + metrics.get('entropy_consequents', 0)\n",
    "                        \n",
    "                        diverse_measures = sorted(diversity_values.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "                        diverse_measures = [m[0] for m in diverse_measures]\n",
    "                    \n",
    "                    dataset_recommendations[name] = {\n",
    "                        'top_consensus_rules': top_consensus_rules,\n",
    "                        'unique_measures': unique_measures,\n",
    "                        'diverse_measures': diverse_measures\n",
    "                    }\n",
    "    \n",
    "    measure_recommendations = {}\n",
    "    for measure in summary.index:\n",
    "        strengths = []\n",
    "        weaknesses = []\n",
    "        \n",
    "        stability = summary.loc[measure, 'Stability Score']\n",
    "        if not np.isnan(stability):\n",
    "            if stability > 0.7:\n",
    "                strengths.append(\"High stability across datasets\")\n",
    "            elif stability < 0.4:\n",
    "                weaknesses.append(\"Low stability across different datasets\")\n",
    "        \n",
    "        diversity = summary.loc[measure, 'Avg Diversity']\n",
    "        if not np.isnan(diversity):\n",
    "            if diversity > 1.5:\n",
    "                strengths.append(\"Discovers diverse and novel rules\")\n",
    "            elif diversity < 0.8:\n",
    "                weaknesses.append(\"Tends to focus on similar rules\")\n",
    "        \n",
    "        correlation = summary.loc[measure, 'Avg Correlation']\n",
    "        if not np.isnan(correlation):\n",
    "            if correlation < 0.3:\n",
    "                strengths.append(\"Provides a unique perspective\")\n",
    "            elif correlation > 0.7:\n",
    "                weaknesses.append(\"Highly correlated with other measures\")\n",
    "        \n",
    "        sig_diff = summary.loc[measure, 'Significant Differences']\n",
    "        if not np.isnan(sig_diff):\n",
    "            if sig_diff > 0.7:\n",
    "                strengths.append(\"Statistically different from most other measures\")\n",
    "            elif sig_diff < 0.3:\n",
    "                weaknesses.append(\"Not statistically different from other measures\")\n",
    "        \n",
    "        measure_recommendations[measure] = {\n",
    "            'strengths': strengths,\n",
    "            'weaknesses': weaknesses,\n",
    "            'recommended_for': summary.loc[measure, 'Recommended For']\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'top_measures': {\n",
    "            'stability': top_stability,\n",
    "            'diversity': top_diversity,\n",
    "            'uniqueness': top_uniqueness\n",
    "        },\n",
    "        'dataset_recommendations': dataset_recommendations,\n",
    "        'measure_recommendations': measure_recommendations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba754bd-6a2d-4fd8-9867-3e323aa5fd3f",
   "metadata": {},
   "source": [
    "The `generate_recommendations` function generates recommendations for mining results and analysis measures based on specific metrics. It evaluates various measures across datasets and provides insights on their stability, diversity, correlation, and performance, as well as recommends optimal uses for each measure.\n",
    "\n",
    "### **Processing Steps**\n",
    "\n",
    "1. **Top Measures Identification**:\n",
    "   - The function first calls `create_measure_comparison_summary()` to get a summary of all measures.\n",
    "   - It then identifies the top 3 measures based on:\n",
    "     - **Stability**: Measures with the highest stability scores.\n",
    "     - **Diversity**: Measures with the highest average diversity.\n",
    "     - **Uniqueness**: Measures with the lowest average correlation (indicating uniqueness).\n",
    "\n",
    "2. **Dataset-Specific Recommendations**:\n",
    "   - For each dataset (`mining_results`), the function checks if it contains relevant information:\n",
    "     - **Top Consensus Rules**: For each dataset, it ranks rules by their measures and selects the top 5 based on their average rank. The top rules are displayed with their antecedents, consequents, support, and confidence.\n",
    "     - **Unique Measures**: It identifies the top 3 measures with the lowest correlation (i.e., the most unique) by calculating the average correlation with all other measures.\n",
    "     - **Diverse Measures**: It selects the top 3 measures with the highest diversity values, based on the entropy of antecedents and consequents.\n",
    "\n",
    "3. **Measure-Specific Recommendations**:\n",
    "   - For each measure, the function identifies its strengths and weaknesses based on:\n",
    "     - **Stability**: High stability indicates that the measure performs consistently across datasets. Low stability suggests that the measure’s performance may vary significantly across datasets.\n",
    "     - **Diversity**: High diversity suggests that the measure identifies novel or diverse rules, while low diversity means the measure focuses on a narrow set of similar rules.\n",
    "     - **Correlation**: Low correlation with other measures implies a unique perspective, while high correlation indicates that the measure may be too similar to others.\n",
    "     - **Statistical Significance**: Measures that show significant differences compared to others are highlighted as strong, while those with little to no statistical difference are noted as weak.\n",
    "   - The function also assigns recommendations for how to use each measure based on the analysis:\n",
    "     - **High Stability**: \"Cross-Dataset Analysis\"\n",
    "     - **Low Stability**: \"Dataset-Specific Analysis\"\n",
    "     - **High Diversity**: \"Discovering Diverse Rules\"\n",
    "     - **Low Diversity**: \"Finding Core Patterns\"\n",
    "     - **Low Correlation**: \"Unique Perspective\"\n",
    "     - **High Correlation**: \"Consensus Measure\"\n",
    "     - **High Statistical Significance**: \"Statistically Different from Other Measures\"\n",
    "     - **Low Statistical Significance**: \"Not Statistically Different\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f98b47b8-bf2f-43bf-a8b2-280c2aa4722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final recommendations...\n",
      "\n",
      "Top Measures by Criteria:\n",
      "Stability: jaccard, odds_ratio, all_confidence\n",
      "Diversity: gini_index, ps, leverage\n",
      "Uniqueness: confidence, conviction, support\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating final recommendations...\")\n",
    "recommendations = generate_recommendations()\n",
    "\n",
    "print(\"\\nTop Measures by Criteria:\")\n",
    "print(f\"Stability: {', '.join(recommendations['top_measures']['stability'])}\")\n",
    "print(f\"Diversity: {', '.join(recommendations['top_measures']['diversity'])}\")\n",
    "print(f\"Uniqueness: {', '.join(recommendations['top_measures']['uniqueness'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a0fce4b-c6be-452a-961c-80bc18d80522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset-Specific Recommendations:\n",
      "\n",
      "Adult Census:\n",
      "Unique Perspective Measures: confidence, support, conviction\n",
      "Diverse Rule Discovery Measures: zhangs_metric, kulczynski, all_confidence\n",
      "Top Consensus Rules:\n",
      "  1. {'relationship_Husband'} => {'marital-status_Married-civ-spouse', 'sex_Male'} [support=0.405, confidence=0.999]\n",
      "  2. {'relationship_Husband'} => {'marital-status_Married-civ-spouse'} [support=0.405, confidence=0.999]\n",
      "  3. {'sex_Male', 'relationship_Husband'} => {'marital-status_Married-civ-spouse'} [support=0.405, confidence=0.999]\n",
      "  4. {'capital-gain_Q1', 'relationship_Husband'} => {'marital-status_Married-civ-spouse'} [support=0.400, confidence=0.999]\n",
      "  5. {'capital-loss_Q1', 'relationship_Husband'} => {'marital-status_Married-civ-spouse'} [support=0.379, confidence=0.999]\n",
      "\n",
      "Mushroom:\n",
      "Unique Perspective Measures: conviction, support, confidence\n",
      "Diverse Rule Discovery Measures: gini_index, leverage, ps\n",
      "Top Consensus Rules:\n",
      "  1. {'18_o', '4_t'} => {'19_p'} [support=0.379, confidence=1.000]\n",
      "  2. {'10_t', '4_f'} => {'19_e'} [support=0.307, confidence=1.000]\n",
      "  3. {'7_c', '19_e'} => {'20_w'} [support=0.240, confidence=1.000]\n",
      "  4. {'11_?', '19_e'} => {'20_w'} [support=0.240, confidence=1.000]\n",
      "  5. {'7_c', '19_e'} => {'11_?'} [support=0.240, confidence=1.000]\n",
      "\n",
      "Bank Marketing:\n",
      "Unique Perspective Measures: confidence, conviction, support\n",
      "Diverse Rule Discovery Measures: lift, leverage, ps\n",
      "Top Consensus Rules:\n",
      "  1. {'marital_married', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.497, confidence=1.000]\n",
      "  2. {'contact_cellular', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.481, confidence=1.000]\n",
      "  3. {'housing_yes', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.442, confidence=1.000]\n",
      "  4. {'education_secondary', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.419, confidence=1.000]\n",
      "  5. {'housing_no', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.375, confidence=1.000]\n",
      "\n",
      "German Credit:\n",
      "Unique Perspective Measures: confidence, support, conviction\n",
      "Diverse Rule Discovery Measures: klosgen, zhangs_metric, gini_index\n",
      "Top Consensus Rules:\n",
      "  1. {'credit_history_A32'} => {'number_credits_Q1'} [support=0.478, confidence=0.902]\n",
      "  2. {'credit_history_A32', 'foreign_worker_A201'} => {'number_credits_Q1'} [support=0.459, confidence=0.900]\n",
      "  3. {'credit_history_A32', 'other_installment_plans_A143'} => {'number_credits_Q1'} [support=0.415, confidence=0.918]\n",
      "  4. {'credit_history_A32', 'other_debtors_A101'} => {'number_credits_Q1'} [support=0.427, confidence=0.905]\n",
      "  5. {'credit_history_A32', 'people_liable_Q1'} => {'number_credits_Q1'} [support=0.417, confidence=0.903]\n",
      "\n",
      "House Prices:\n",
      "Unique Perspective Measures: confidence, conviction, support\n",
      "Diverse Rule Discovery Measures: collective_strength, odds_ratio, confidence\n",
      "Top Consensus Rules:\n",
      "  1. {'HouseStyle_1Story'} => {'2ndFlrSF_Q1', 'Utilities_AllPub'} [support=0.497, confidence=0.999]\n",
      "  2. {'HouseStyle_1Story', 'Utilities_AllPub'} => {'2ndFlrSF_Q1'} [support=0.497, confidence=0.999]\n",
      "  3. {'HouseStyle_1Story'} => {'2ndFlrSF_Q1'} [support=0.497, confidence=0.999]\n",
      "  4. {'PoolArea_Q1', 'HouseStyle_1Story'} => {'2ndFlrSF_Q1'} [support=0.496, confidence=0.999]\n",
      "  5. {'MiscVal_Q1', 'HouseStyle_1Story'} => {'2ndFlrSF_Q1'} [support=0.496, confidence=0.999]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset-Specific Recommendations:\")\n",
    "for name, recs in recommendations['dataset_recommendations'].items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Unique Perspective Measures: {', '.join(recs['unique_measures'])}\")\n",
    "    if 'diverse_measures' in recs and recs['diverse_measures']:\n",
    "        print(f\"Diverse Rule Discovery Measures: {', '.join(recs['diverse_measures'])}\")\n",
    "    print(\"Top Consensus Rules:\")\n",
    "    for i, rule in enumerate(recs['top_consensus_rules']):\n",
    "        print(f\"  {i+1}. {rule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c48d67ee-2e64-4839-b4e9-21b558b7a96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Measure-Specific Recommendations:\n",
      "\n",
      "odds_ratio:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "support:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "  + Provides a unique perspective\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules, Unique Perspective\n",
      "\n",
      "gini_index:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules\n",
      "\n",
      "zhangs_metric:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "all_confidence:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "ps:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "klosgen:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "cosine:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "lift:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "conviction:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "  + Provides a unique perspective\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules, Unique Perspective\n",
      "\n",
      "kulczynski:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules\n",
      "\n",
      "confidence:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "  + Provides a unique perspective\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules, Unique Perspective\n",
      "\n",
      "leverage:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "jaccard:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "collective_strength:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMeasure-Specific Recommendations:\")\n",
    "for measure, recs in recommendations['measure_recommendations'].items():\n",
    "    print(f\"\\n{measure}:\")\n",
    "    if recs['strengths']:\n",
    "        print(\"Strengths:\")\n",
    "        for strength in recs['strengths']:\n",
    "            print(f\"  + {strength}\")\n",
    "    if recs['weaknesses']:\n",
    "        print(\"Weaknesses:\")\n",
    "        for weakness in recs['weaknesses']:\n",
    "            print(f\"  - {weakness}\")\n",
    "    if recs['recommended_for']:\n",
    "        print(f\"Recommended For: {recs['recommended_for']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055bd4ec-de66-42ef-8322-2eb431ef4ff5",
   "metadata": {},
   "source": [
    "The analysis of association rule interestingness measures across multiple datasets reveals several important insights:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **No Universal Best Measure**: No single measure consistently outperforms others across all datasets and evaluation criteria.\n",
    "\n",
    "2. **Complementary Strengths**: Different measures capture different aspects of interestingness, with some focusing on statistical significance, others on diversity, and others on rule novelty.\n",
    "\n",
    "3. **Dataset Dependency**: The effectiveness of interestingness measures varies significantly based on dataset.\n",
    "\n",
    "4. **Stability Considerations**: Measures with high stability scores (particularly Jaccard, Odds ratio, and All-confidence) provide more consistent results across different datasets.\n",
    "\n",
    "5. **Diversity Value**: Measures promoting diverse rule discovery (such as Added Value, Certainty Factor, and Jaccard) help uncover novel patterns that might be missed by conventional measures.\n",
    "\n",
    "6. **Statistical Significance**: The Wilcoxon tests revealed that all of the measures have other measures that are not signifacantly different from them - makes sense as many measures rely on each other.\n",
    "\n",
    "7. **Clustering of Measures**: For most of the datasets the measures divide to 3 clusters - where Support, Confidence and Lift each in different cluster - makes sense as they are the fundemental measures that indicate different aspects of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
