{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a9f84a-a7aa-4841-b1f6-c2258f939876",
   "metadata": {},
   "source": [
    "# Comparative Analysis of Interestingness Measures in Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a350b-4bf8-487e-9e25-af2a1f45667f",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This notebook was developed with assistance from Claude. Claude helped with content generation, code suggestions, and structure organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddcd58-209b-4eb9-96b6-25d90587e190",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Data Loading](#setup)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Association Rule Mining](#mining)\n",
    "5. [Interestingness Measures Implementation](#measures)\n",
    "6. [Comparative Analysis](#analysis)\n",
    "7. [Cross-Dataset Stability Analysis](#cross-dataset)\n",
    "8. [Statistical Robustness Testing](#statistical)\n",
    "9. [Visualization of Results](#visualization)\n",
    "10. [Conclusions and Recommendations](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c38d6-2953-40ba-bf17-9c61dbfd8f41",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b1b0d-c379-48a4-84b9-bd7a81257229",
   "metadata": {},
   "source": [
    "This notebook implements the research proposal on improving the evaluation of association rules in pattern mining by comparing existing interestingness measures. We'll analyze five datasets to determine which measures work best for different data types and provide visual comparisons of their effectiveness.\n",
    "\n",
    "**Research Goals:**\n",
    "- Compare the effectiveness of existing interestingness measures\n",
    "- Determine which measures work best for different types of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7a1cd-14e5-4b23-8e15-848aaacdfe5b",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d1092-6e7f-4eb7-ac9a-cbdd39e26768",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries and loading our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66195fec-dd87-46f8-afac-5492196b62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from scipy.stats import spearmanr, wilcoxon\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "colors = sns.color_palette(\"viridis\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb006b74-075a-4542-a7a3-9b9ea1f7684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, dataset_type):\n",
    "    if dataset_type == 'adult':\n",
    "        columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                   'marital-status', 'occupation', 'relationship', 'race', \n",
    "                   'sex', 'capital-gain', 'capital-loss', 'hours-per-week', \n",
    "                   'native-country', 'income']\n",
    "        return pd.read_csv(filename, names=columns, sep=', ', engine='python')\n",
    "    \n",
    "    elif dataset_type == 'mushroom':\n",
    "        # Mushroom dataset has no header\n",
    "        return pd.read_csv(filename, header=None)\n",
    "    \n",
    "    elif dataset_type == 'bank':\n",
    "        return pd.read_csv(filename, sep=';')\n",
    "    \n",
    "    elif dataset_type == 'german':\n",
    "        # German credit data\n",
    "        columns = ['status', 'duration', 'credit_history', 'purpose', 'amount',\n",
    "                   'savings', 'employment_duration', 'installment_rate', 'personal_status_sex',\n",
    "                   'other_debtors', 'residence_since', 'property', 'age', 'other_installment_plans',\n",
    "                   'housing', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker',\n",
    "                   'credit_risk']\n",
    "        return pd.read_csv(filename, sep=' ', names=columns)\n",
    "    \n",
    "    elif dataset_type == 'house_prices':\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset type: {dataset_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e685285d-4fdf-4e8c-a345-2278c61af836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "try:\n",
    "    adult_df = load_dataset('Datasets/adult.data', 'adult')\n",
    "    mushroom_df = load_dataset('Datasets/agaricus-lepiota.data', 'mushroom')\n",
    "    bank_df = load_dataset('Datasets/bank-full.csv', 'bank')\n",
    "    german_df = load_dataset('Datasets/german.data', 'german')\n",
    "    house_df = load_dataset('Datasets/house_prices.csv', 'house_prices')\n",
    "    \n",
    "    datasets = {\n",
    "        'Adult Census': adult_df,\n",
    "        'Mushroom': mushroom_df,\n",
    "        'Bank Marketing': bank_df,\n",
    "        'German Credit': german_df,\n",
    "        'House Prices': house_df\n",
    "    }\n",
    "    \n",
    "    print(\"All datasets loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a60379-ff8e-46be-9322-615755493dc6",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7298d9d-7de0-4aa5-8c5e-70f49eda12cd",
   "metadata": {},
   "source": [
    "We need to convert our datasets into a transaction format suitable for association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c74c6d6-2736-4959-8a60-d9f3d49e3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df, categorical_cols=None, numerical_cols=None, binary_cols=None, target_col=None):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        if df_copy[col].dtype == 'object':\n",
    "            df_copy[col] = df_copy[col].fillna('Unknown')\n",
    "        else:\n",
    "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
    "    \n",
    "    df_transformed = pd.DataFrame()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            if col in df_copy.columns:\n",
    "                # One-hot encode categorical variables\n",
    "                df_transformed = pd.concat([\n",
    "                    df_transformed, \n",
    "                    pd.get_dummies(df_copy[col], prefix=col, drop_first=False)\n",
    "                ], axis=1)\n",
    "    \n",
    "    if numerical_cols:\n",
    "        for col in numerical_cols:\n",
    "            if col in df_copy.columns:\n",
    "                # Bin numerical variables into 5 categories\n",
    "                try:\n",
    "                    df_copy[f\"{col}_binned\"] = pd.qcut(\n",
    "                        df_copy[col], q=5, duplicates='drop', \n",
    "                        labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                    )\n",
    "                except:\n",
    "                    df_copy[f\"{col}_binned\"] = pd.cut(\n",
    "                        df_copy[col], bins=5, duplicates='drop', \n",
    "                        labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                    )\n",
    "                # One-hot encode binned variables\n",
    "                df_transformed = pd.concat([\n",
    "                    df_transformed, \n",
    "                    pd.get_dummies(df_copy[f\"{col}_binned\"], prefix=col, drop_first=False)\n",
    "                ], axis=1)\n",
    "    \n",
    "    if binary_cols:\n",
    "        for col in binary_cols:\n",
    "            if col in df_copy.columns:\n",
    "                df_transformed[col] = df_copy[col]\n",
    "    \n",
    "    if target_col and target_col in df_copy.columns:\n",
    "        if df_copy[target_col].dtype == 'object':\n",
    "            df_transformed = pd.concat([\n",
    "                df_transformed, \n",
    "                pd.get_dummies(df_copy[target_col], prefix=target_col, drop_first=False)\n",
    "            ], axis=1)\n",
    "        else:\n",
    "            try:\n",
    "                df_copy[f\"{target_col}_binned\"] = pd.qcut(\n",
    "                    df_copy[target_col], q=5, duplicates='drop', \n",
    "                    labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                )\n",
    "            except:\n",
    "                df_copy[f\"{target_col}_binned\"] = pd.cut(\n",
    "                    df_copy[target_col], bins=5, duplicates='drop', \n",
    "                    labels=[f\"Q{i}\" for i in range(1, 6)]\n",
    "                )\n",
    "            df_transformed = pd.concat([\n",
    "                df_transformed, \n",
    "                pd.get_dummies(df_copy[f\"{target_col}_binned\"], prefix=target_col, drop_first=False)\n",
    "            ], axis=1)\n",
    "    \n",
    "    df_transformed = df_transformed.astype(int)\n",
    "    \n",
    "    transactions = df_transformed.apply(\n",
    "        lambda row: row.index[row == 1].tolist(),\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "    \n",
    "    return df_transformed, transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8f99299-d288-4696-a0b9-bfdba131ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_specs = {\n",
    "    'Adult Census': {\n",
    "        'categorical_cols': ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                            'relationship', 'race', 'sex', 'native-country'],\n",
    "        'numerical_cols': ['age', 'fnlwgt', 'education-num', 'capital-gain', \n",
    "                          'capital-loss', 'hours-per-week'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'income'\n",
    "    },\n",
    "    'Mushroom': {\n",
    "        'categorical_cols': list(range(1, 23)),\n",
    "        'numerical_cols': [],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 0\n",
    "    },\n",
    "    'Bank Marketing': {\n",
    "        'categorical_cols': ['job', 'marital', 'education', 'default', 'housing', \n",
    "                            'loan', 'contact', 'month', 'poutcome'],\n",
    "        'numerical_cols': ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'y'\n",
    "    },\n",
    "    'German Credit': {\n",
    "        'categorical_cols': ['status', 'credit_history', 'purpose', 'savings', \n",
    "                            'employment_duration', 'personal_status_sex', 'other_debtors',\n",
    "                            'property', 'other_installment_plans', 'housing', 'job',\n",
    "                            'telephone', 'foreign_worker'],\n",
    "        'numerical_cols': ['duration', 'amount', 'installment_rate', 'residence_since', \n",
    "                          'age', 'number_credits', 'people_liable'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'credit_risk'\n",
    "    },\n",
    "    'House Prices': {\n",
    "        'categorical_cols': ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', \n",
    "                            'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', \n",
    "                            'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \n",
    "                            'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \n",
    "                            'MasVnrType', 'Foundation', 'Heating', 'CentralAir', \n",
    "                            'Electrical', 'Functional', 'GarageType', 'PavedDrive', \n",
    "                            'Fence', 'MiscFeature', 'SaleType', 'SaleCondition'],\n",
    "        'numerical_cols': ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', \n",
    "                          'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', \n",
    "                          'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n",
    "                          '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \n",
    "                          'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n",
    "                          'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', \n",
    "                          'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n",
    "                          'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n",
    "                          'MiscVal', 'MoSold', 'YrSold'],\n",
    "        'binary_cols': [],\n",
    "        'target_col': 'SalePrice'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5df8d2ec-0ff6-44e3-ad5a-14db47e370bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0129abcc0ef41debc0033fc92cf07c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Adult Census dataset: 134 features\n",
      "Preprocessed Mushroom dataset: 117 features\n",
      "Preprocessed Bank Marketing dataset: 81 features\n",
      "Preprocessed German Credit dataset: 94 features\n",
      "Preprocessed House Prices dataset: 374 features\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = {}\n",
    "print(\"Preprocessing datasets...\")\n",
    "\n",
    "for name, df in tqdm(datasets.items()):\n",
    "    specs = preprocessing_specs.get(name, {\n",
    "        'categorical_cols': df.select_dtypes(include=['object']).columns.tolist(),\n",
    "        'numerical_cols': df.select_dtypes(include=['int64', 'float64']).columns.tolist(),\n",
    "        'binary_cols': [],\n",
    "        'target_col': None\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        df_transformed, transactions = preprocess_dataset(\n",
    "            df, \n",
    "            categorical_cols=specs['categorical_cols'],\n",
    "            numerical_cols=specs['numerical_cols'],\n",
    "            binary_cols=specs['binary_cols'],\n",
    "            target_col=specs['target_col']\n",
    "        )\n",
    "        \n",
    "        preprocessed_data[name] = {\n",
    "            'transformed_df': df_transformed,\n",
    "            'transactions': transactions,\n",
    "            'original_df': df\n",
    "        }\n",
    "        print(f\"Preprocessed {name} dataset: {df_transformed.shape[1]} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5f892-cf71-4a6c-a9d8-a2a979791757",
   "metadata": {},
   "source": [
    "## 4. Association Rule Mining <a id=\"mining\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d27eb2-93f8-448b-9542-a118e8c0833a",
   "metadata": {},
   "source": [
    "Now we'll perform association rule mining using the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e9af994-9c4f-4ece-bbfe-44982e5abfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_association_rules(transactions, min_support=0.1, min_confidence=0.5, max_length=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True, max_len=max_length, verbose=0)\n",
    "    \n",
    "    if not frequent_itemsets.empty:\n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        end_time = time.time()\n",
    "        print(f\"Mining completed in {end_time - start_time:.2f} seconds. Found {len(rules)} rules.\")\n",
    "        return frequent_itemsets, rules\n",
    "    else:\n",
    "        print(\"No frequent itemsets found with the current parameters.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fdefa5d-c0e6-4b98-9a6e-a0da5ac631fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_params = {\n",
    "    'Adult Census': {'min_support': 0.05, 'min_confidence': 0.5, 'max_length': 3},\n",
    "    'Mushroom': {'min_support': 0.2, 'min_confidence': 0.7, 'max_length': 3},\n",
    "    'Bank Marketing': {'min_support': 0.05, 'min_confidence': 0.5, 'max_length': 3},\n",
    "    'German Credit': {'min_support': 0.1, 'min_confidence': 0.6, 'max_length': 3},\n",
    "    'House Prices': {'min_support': 0.1, 'min_confidence': 0.5, 'max_length': 3}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8df002a5-cdd1-437e-8fc5-24b3bbf05ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining association rules...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad4f303674d4a5bbf2db4d679f14edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mining rules for Adult Census...\n",
      "Mining completed in 2.61 seconds. Found 5076 rules.\n",
      "Found 2433 frequent itemsets and 5076 rules\n",
      "\n",
      "Mining rules for Mushroom...\n",
      "Mining completed in 0.24 seconds. Found 3232 rules.\n",
      "Found 1594 frequent itemsets and 3232 rules\n",
      "\n",
      "Mining rules for Bank Marketing...\n",
      "Mining completed in 3.40 seconds. Found 7827 rules.\n",
      "Found 3669 frequent itemsets and 7827 rules\n",
      "\n",
      "Mining rules for German Credit...\n",
      "Mining completed in 0.07 seconds. Found 5089 rules.\n",
      "Found 3036 frequent itemsets and 5089 rules\n",
      "\n",
      "Mining rules for House Prices...\n",
      "Mining completed in 5.85 seconds. Found 353412 rules.\n",
      "Found 111472 frequent itemsets and 353412 rules\n"
     ]
    }
   ],
   "source": [
    "mining_results = {}\n",
    "print(\"Mining association rules...\")\n",
    "\n",
    "for name, data in tqdm(preprocessed_data.items()):\n",
    "    params = mining_params.get(name, {'min_support': 0.1, 'min_confidence': 0.5, 'max_length': 3})\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nMining rules for {name}...\")\n",
    "        frequent_itemsets, rules = mine_association_rules(\n",
    "            data['transactions'],\n",
    "            min_support=params['min_support'],\n",
    "            min_confidence=params['min_confidence'],\n",
    "            max_length=params['max_length']\n",
    "        )\n",
    "        \n",
    "        if not rules.empty:\n",
    "            mining_results[name] = {\n",
    "                'frequent_itemsets': frequent_itemsets,\n",
    "                'rules': rules\n",
    "            }\n",
    "            print(f\"Found {len(frequent_itemsets)} frequent itemsets and {len(rules)} rules\")\n",
    "        else:\n",
    "            print(f\"No rules found for {name} dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error mining rules for {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a9d14-3860-40b7-a02b-9af4cfe86faf",
   "metadata": {},
   "source": [
    "## 5. Interestingness Measures Implementation <a id=\"measures\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b402e2-b071-41f9-b0a1-08eef9f436bd",
   "metadata": {},
   "source": [
    "We'll implement a range of interestingness measures for evaluating association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03f68915-4b5f-47a4-8f01-8d1d76ca0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_additional_measures(rules):\n",
    "    rules_copy = rules.copy()\n",
    "    \n",
    "    # Standard measures already in the dataframe:\n",
    "    # - support\n",
    "    # - confidence\n",
    "    # - lift\n",
    "    \n",
    "    # Calculate additional measures\n",
    "    \n",
    "    # Conviction: measure of implication strength\n",
    "    rules_copy['conviction'] = np.where(\n",
    "        (1 - rules_copy['confidence']) == 0, \n",
    "        float('inf'), \n",
    "        (1 - rules_copy['antecedent support']) / (1 - rules_copy['confidence'])\n",
    "    )\n",
    "    \n",
    "    # Leverage (Piatetsky-Shapiro): difference between observed and expected frequency\n",
    "    rules_copy['leverage'] = rules_copy['support'] - (rules_copy['antecedent support'] * rules_copy['consequent support'])\n",
    "    \n",
    "    # Jaccard coefficient: similarity measure\n",
    "    rules_copy['jaccard'] = rules_copy['support'] / (\n",
    "        rules_copy['antecedent support'] + rules_copy['consequent support'] - rules_copy['support']\n",
    "    )\n",
    "    \n",
    "    # Cosine: normalized measure of co-occurrence\n",
    "    rules_copy['cosine'] = rules_copy['support'] / np.sqrt(\n",
    "        rules_copy['antecedent support'] * rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    # Kulczynski: average of two conditional probabilities\n",
    "    rules_copy['kulczynski'] = 0.5 * (\n",
    "        rules_copy['confidence'] + rules_copy['support'] / rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    # All-confidence: minimum of the two confidence values\n",
    "    rules_copy['all_confidence'] = rules_copy['support'] / np.maximum(\n",
    "        rules_copy['antecedent support'], rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    # Collective strength\n",
    "    # P(violation in real data) / P(violation in independent case)\n",
    "    p_v_real = 1 - rules_copy['support'] - (\n",
    "        rules_copy['antecedent support'] * (1 - rules_copy['consequent support']) + \n",
    "        (1 - rules_copy['antecedent support']) * rules_copy['consequent support']\n",
    "    )\n",
    "    p_v_ind = 1 - (\n",
    "        rules_copy['antecedent support'] * rules_copy['consequent support'] + \n",
    "        (1 - rules_copy['antecedent support']) * (1 - rules_copy['consequent support'])\n",
    "    )\n",
    "    rules_copy['collective_strength'] = np.where(\n",
    "        p_v_ind == 0, \n",
    "        float('inf'), \n",
    "        (1 - p_v_real) / (1 - p_v_ind) * p_v_ind / p_v_real\n",
    "    )\n",
    "    \n",
    "    # Gini index\n",
    "    p_xy = rules_copy['support']\n",
    "    p_x = rules_copy['antecedent support']\n",
    "    p_y = rules_copy['consequent support']\n",
    "    p_not_x = 1 - p_x\n",
    "    p_not_y = 1 - p_y\n",
    "    p_x_y = p_xy / p_x  # P(Y|X)\n",
    "    p_x_not_y = (p_x - p_xy) / p_x  # P(¬Y|X)\n",
    "    p_not_x_y = (p_y - p_xy) / p_not_x  # P(Y|¬X)\n",
    "    p_not_x_not_y = (1 - p_x - p_y + p_xy) / p_not_x  # P(¬Y|¬X)\n",
    "    \n",
    "    gini_x = p_x * (p_x_y**2 + p_x_not_y**2) + p_not_x * (p_not_x_y**2 + p_not_x_not_y**2)\n",
    "    gini_y = p_y**2 + p_not_y**2\n",
    "    \n",
    "    rules_copy['gini_index'] = gini_x - gini_y\n",
    "    \n",
    "    # Piatetsky-Shapiro: deviation from independence\n",
    "    rules_copy['ps'] = rules_copy['support'] - (rules_copy['antecedent support'] * rules_copy['consequent support'])\n",
    "    \n",
    "    # Odds ratio: ratio of odds of occurrence\n",
    "    p_xy = rules_copy['support']\n",
    "    p_x = rules_copy['antecedent support']\n",
    "    p_y = rules_copy['consequent support']\n",
    "    p_not_xy = 1 - p_x - p_y + p_xy\n",
    "    rules_copy['odds_ratio'] = np.where(\n",
    "        (p_x - p_xy) * (p_y - p_xy) == 0, \n",
    "        float('inf'), \n",
    "        (p_xy * p_not_xy) / ((p_x - p_xy) * (p_y - p_xy))\n",
    "    )\n",
    "    \n",
    "    # Klosgen: combines support and confidence\n",
    "    rules_copy['klosgen'] = np.sqrt(rules_copy['support']) * (\n",
    "        rules_copy['confidence'] - rules_copy['consequent support']\n",
    "    )\n",
    "    \n",
    "    return rules_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f3323cb-4547-409a-85ee-c5b7eda35937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating interestingness measures for Adult Census...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for Mushroom...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for Bank Marketing...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for German Credit...\n",
      "Calculated 10 additional measures\n",
      "Calculating interestingness measures for House Prices...\n",
      "Calculated 10 additional measures\n"
     ]
    }
   ],
   "source": [
    "for name, result in mining_results.items():\n",
    "    if 'rules' in result and not result['rules'].empty:\n",
    "        print(f\"Calculating interestingness measures for {name}...\")\n",
    "        try:\n",
    "            mining_results[name]['rules_with_measures'] = calculate_additional_measures(result['rules'])\n",
    "            print(f\"Calculated {len(mining_results[name]['rules_with_measures'].columns) - 9} additional measures\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating measures for {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5a654-6507-4f84-9881-19a3bf172ee0",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis <a id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ce575-9d7d-45ed-9986-41dccb08f4cf",
   "metadata": {},
   "source": [
    "Now, let's analyze the effectiveness of different interestingness measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e248609-6e11-4b75-b5af-4263a6f40626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_measure_correlations(rules_df):\n",
    "    measure_cols = [col for col in rules_df.columns if col not in [\n",
    "        'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "    ]]\n",
    "    \n",
    "    correlation_matrix = rules_df[measure_cols].corr(method='spearman')\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a45fc5b-c9f8-4981-9006-13cad2964b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rule_rankings(rules_df, top_n=50):\n",
    "    measure_cols = [col for col in rules_df.columns if col not in [\n",
    "        'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "    ]]\n",
    "    \n",
    "    top_rules = {}\n",
    "    for measure in measure_cols:\n",
    "        sorted_rules = rules_df.sort_values(by=measure, ascending=False)\n",
    "        top_rules[measure] = sorted_rules.head(top_n)\n",
    "    \n",
    "    jaccard_similarity = pd.DataFrame(index=measure_cols, columns=measure_cols, dtype=float)\n",
    "    for m1 in measure_cols:\n",
    "        for m2 in measure_cols:\n",
    "            top_m1 = set(top_rules[m1].index)\n",
    "            top_m2 = set(top_rules[m2].index)\n",
    "            intersection = len(top_m1.intersection(top_m2))\n",
    "            union = len(top_m1.union(top_m2))\n",
    "            jaccard_similarity.loc[m1, m2] = intersection / union if union > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'top_rules': top_rules,\n",
    "        'jaccard_similarity': jaccard_similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cd49b7c-dce8-4879-95ea-9544cf041d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rule_diversity(rules_df, measure, top_n=50):\n",
    "    sorted_rules = rules_df.sort_values(by=measure, ascending=False)\n",
    "    top_rules = sorted_rules.head(top_n)\n",
    "    \n",
    "    antecedent_items = set()\n",
    "    for items in top_rules['antecedents']:\n",
    "        antecedent_items.update(items)\n",
    "    \n",
    "    consequent_items = set()\n",
    "    for items in top_rules['consequents']:\n",
    "        consequent_items.update(items)\n",
    "    \n",
    "    support_range = (top_rules['support'].min(), top_rules['support'].max())\n",
    "    support_std = top_rules['support'].std()\n",
    "    \n",
    "    confidence_range = (top_rules['confidence'].min(), top_rules['confidence'].max())\n",
    "    confidence_std = top_rules['confidence'].std()\n",
    "    \n",
    "    # Calculate entropy-based diversity\n",
    "    # Higher entropy means more diverse rules\n",
    "    unique_antecedents = top_rules['antecedents'].apply(frozenset).value_counts()\n",
    "    p_antecedents = unique_antecedents / unique_antecedents.sum()\n",
    "    entropy_antecedents = -np.sum(p_antecedents * np.log2(p_antecedents))\n",
    "    \n",
    "    unique_consequents = top_rules['consequents'].apply(frozenset).value_counts()\n",
    "    p_consequents = unique_consequents / unique_consequents.sum()\n",
    "    entropy_consequents = -np.sum(p_consequents * np.log2(p_consequents))\n",
    "    \n",
    "    return {\n",
    "        'num_antecedent_items': len(antecedent_items),\n",
    "        'num_consequent_items': len(consequent_items),\n",
    "        'support_range': support_range,\n",
    "        'support_std': support_std,\n",
    "        'confidence_range': confidence_range,\n",
    "        'confidence_std': confidence_std,\n",
    "        'entropy_antecedents': entropy_antecedents,\n",
    "        'entropy_consequents': entropy_consequents\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca7ce042-1d81-43dd-9aae-24df536521a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing measures for Adult Census dataset...\n",
      "Analysis completed for Adult Census dataset\n",
      "\n",
      "Analyzing measures for Mushroom dataset...\n",
      "Analysis completed for Mushroom dataset\n",
      "\n",
      "Analyzing measures for Bank Marketing dataset...\n",
      "Analysis completed for Bank Marketing dataset\n",
      "\n",
      "Analyzing measures for German Credit dataset...\n",
      "Analysis completed for German Credit dataset\n",
      "\n",
      "Analyzing measures for House Prices dataset...\n",
      "Analysis completed for House Prices dataset\n"
     ]
    }
   ],
   "source": [
    "analysis_results = {}\n",
    "for name, result in mining_results.items():\n",
    "    if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "        print(f\"\\nAnalyzing measures for {name} dataset...\")\n",
    "        \n",
    "        rules_df = result['rules_with_measures']\n",
    "        \n",
    "        try:\n",
    "            correlation_matrix = analyze_measure_correlations(rules_df)\n",
    "            \n",
    "            ranking_analysis = analyze_rule_rankings(rules_df, top_n=min(50, len(rules_df)))\n",
    "            \n",
    "            diversity_analysis = {}\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            \n",
    "            for measure in measure_cols:\n",
    "                diversity_analysis[measure] = analyze_rule_diversity(\n",
    "                    rules_df, measure, top_n=min(50, len(rules_df))\n",
    "                )\n",
    "            \n",
    "            analysis_results[name] = {\n",
    "                'correlation_matrix': correlation_matrix,\n",
    "                'ranking_analysis': ranking_analysis,\n",
    "                'diversity_analysis': diversity_analysis\n",
    "            }\n",
    "            \n",
    "            print(f\"Analysis completed for {name} dataset\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {name} dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea03e3c-4422-4a8f-8b42-f5a1272297d4",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Stability Analysis <a id=\"cross-dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576be238-778c-4f74-a952-cefb698f4e09",
   "metadata": {},
   "source": [
    "Let's analyze the stability of measures across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9418d14a-b9b1-43be-bd33-ea95d23d9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_dataset_stability():\n",
    "    all_measures = []\n",
    "    for name, result in mining_results.items():\n",
    "        if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "            rules_df = result['rules_with_measures']\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            all_measures.extend(measure_cols)\n",
    "    all_measures = list(set(all_measures))\n",
    "    \n",
    "    stability_metrics = {\n",
    "        'correlation_variation': {measure: [] for measure in all_measures},\n",
    "        'diversity_variation': {measure: [] for measure in all_measures}\n",
    "    }\n",
    "    \n",
    "    for measure in all_measures:\n",
    "        correlations_across_datasets = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'correlation_matrix' in result:\n",
    "                corr_matrix = result['correlation_matrix']\n",
    "                if measure in corr_matrix.columns:\n",
    "                    correlations = corr_matrix[measure].drop(measure).values\n",
    "                    correlations_across_datasets.append(correlations)\n",
    "                    \n",
    "        if len(correlations_across_datasets) > 1:\n",
    "            corr_arrays = [np.array(c) for c in correlations_across_datasets if len(c) > 0]\n",
    "            \n",
    "            if len(corr_arrays) > 1 and all(len(c) == len(corr_arrays[0]) for c in corr_arrays):\n",
    "                stacked = np.vstack(corr_arrays)\n",
    "                mean_corr = np.mean(stacked, axis=0)\n",
    "                std_corr = np.std(stacked, axis=0)\n",
    "                cv_corr = np.where(np.abs(mean_corr) > 1e-10, std_corr / np.abs(mean_corr), 0)\n",
    "                stability_metrics['correlation_variation'][measure] = np.mean(cv_corr)\n",
    "\n",
    "    for measure in all_measures:\n",
    "        diversity_metrics = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'diversity_analysis' in result and measure in result['diversity_analysis']:\n",
    "                diversity = result['diversity_analysis'][measure]\n",
    "                metrics = {\n",
    "                    'entropy_antecedents': diversity.get('entropy_antecedents', 0),\n",
    "                    'entropy_consequents': diversity.get('entropy_consequents', 0),\n",
    "                    'support_std': diversity.get('support_std', 0)\n",
    "                }\n",
    "                diversity_metrics.append(metrics)\n",
    "        \n",
    "        if len(diversity_metrics) > 1:\n",
    "            entropy_ant = [d['entropy_antecedents'] for d in diversity_metrics]\n",
    "            entropy_cons = [d['entropy_consequents'] for d in diversity_metrics]\n",
    "            support_std = [d['support_std'] for d in diversity_metrics]\n",
    "            \n",
    "            cv_entropy_ant = np.std(entropy_ant) / np.mean(entropy_ant) if np.mean(entropy_ant) > 0 else 0\n",
    "            cv_entropy_cons = np.std(entropy_cons) / np.mean(entropy_cons) if np.mean(entropy_cons) > 0 else 0\n",
    "            cv_support_std = np.std(support_std) / np.mean(support_std) if np.mean(support_std) > 0 else 0\n",
    "            \n",
    "            stability_metrics['diversity_variation'][measure] = np.mean([\n",
    "                cv_entropy_ant, cv_entropy_cons, cv_support_std\n",
    "            ])\n",
    "    \n",
    "    return stability_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0aa893cc-9919-437c-b867-139b471b560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-dataset stability analysis...\n",
      "Cross-dataset stability analysis completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing cross-dataset stability analysis...\")\n",
    "stability_metrics = analyze_cross_dataset_stability()\n",
    "\n",
    "stability_scores = {}\n",
    "for measure in stability_metrics['correlation_variation']:\n",
    "    if measure in stability_metrics['correlation_variation'] and measure in stability_metrics['diversity_variation']:\n",
    "        corr_var = stability_metrics['correlation_variation'][measure]\n",
    "        div_var = stability_metrics['diversity_variation'][measure]\n",
    "        \n",
    "        if not np.isnan(corr_var) and not np.isnan(div_var):\n",
    "            stability_scores[measure] = 1 - np.mean([\n",
    "                corr_var / max(stability_metrics['correlation_variation'].values()),\n",
    "                div_var / max(stability_metrics['diversity_variation'].values())\n",
    "            ])\n",
    "\n",
    "print(\"Cross-dataset stability analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ae299-9d9e-49d4-a4f2-9003ed0a7293",
   "metadata": {},
   "source": [
    "## 8. Statistical Robustness Testing <a id=\"statistical\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1e868-de35-4ff6-9044-a57efc09f5f6",
   "metadata": {},
   "source": [
    "Let's perform statistical tests to evaluate the robustness of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "676d1adf-388d-408f-9a68-f03dbae1757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_wilcoxon_tests(top_n=50):\n",
    "    all_measures = []\n",
    "    for name, result in mining_results.items():\n",
    "        if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "            rules_df = result['rules_with_measures']\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            all_measures.extend(measure_cols)\n",
    "    all_measures = list(set(all_measures))\n",
    "    \n",
    "    test_results = {\n",
    "        dataset_name: {\n",
    "            'p_values': pd.DataFrame(index=all_measures, columns=all_measures),\n",
    "            'significant_differences': pd.DataFrame(index=all_measures, columns=all_measures)\n",
    "        }\n",
    "        for dataset_name in mining_results.keys()\n",
    "    }\n",
    "    \n",
    "    for dataset_name, result in mining_results.items():\n",
    "        if 'rules_with_measures' not in result or result['rules_with_measures'].empty:\n",
    "            continue\n",
    "        \n",
    "        rules_df = result['rules_with_measures']\n",
    "        dataset_measures = [col for col in rules_df.columns if col not in [\n",
    "            'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "        ]]\n",
    "        \n",
    "        ranks = pd.DataFrame(index=rules_df.index)\n",
    "        for measure in dataset_measures:\n",
    "            ranks[measure] = rules_df[measure].rank(ascending=False)\n",
    "        \n",
    "        for m1 in dataset_measures:\n",
    "            for m2 in dataset_measures:\n",
    "                if m1 != m2:\n",
    "                    try:\n",
    "                        stat, p_value = wilcoxon(ranks[m1], ranks[m2])\n",
    "                        \n",
    "                        test_results[dataset_name]['p_values'].loc[m1, m2] = p_value\n",
    "                        \n",
    "                        test_results[dataset_name]['significant_differences'].loc[m1, m2] = p_value < 0.05\n",
    "                    except Exception:\n",
    "                        test_results[dataset_name]['p_values'].loc[m1, m2] = 1.0\n",
    "                        test_results[dataset_name]['significant_differences'].loc[m1, m2] = False\n",
    "                else:\n",
    "                    test_results[dataset_name]['p_values'].loc[m1, m2] = 1.0\n",
    "                    test_results[dataset_name]['significant_differences'].loc[m1, m2] = False\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab6e9bfa-917a-491c-ab2c-436cd40ab39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing statistical robustness testing...\n",
      "Statistical testing completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing statistical robustness testing...\")\n",
    "statistical_tests = perform_wilcoxon_tests()\n",
    "print(\"Statistical testing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9c469-49af-4276-9844-cf5e4f5475b2",
   "metadata": {},
   "source": [
    "## 9. Visualization of Results <a id=\"visualization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b1034-bbd6-466c-ad96-203e856f0f7e",
   "metadata": {},
   "source": [
    "Now, let's visualize our results to better understand the relationships and effectiveness of different measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c89150f-fbb8-4a0e-bf07-716778be8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_measure_correlations(dataset_name, correlation_matrix):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        correlation_matrix, \n",
    "        mask=mask,\n",
    "        annot=True, \n",
    "        fmt=\".2f\", \n",
    "        cmap='coolwarm', \n",
    "        vmin=-1, \n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Spearman Rank Correlation Between Measures ({dataset_name})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef067566-fd25-4d5e-a139-eb5d746d48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_measure_clusters(dataset_name, correlation_matrix):\n",
    "    distance_matrix = 1 - np.abs(correlation_matrix)\n",
    "    \n",
    "    linkage_matrix = scipy.cluster.hierarchy.linkage(\n",
    "        scipy.spatial.distance.squareform(distance_matrix), \n",
    "        method='average'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    scipy.cluster.hierarchy.dendrogram(\n",
    "        linkage_matrix,\n",
    "        labels=correlation_matrix.columns,\n",
    "        leaf_font_size=12,\n",
    "        color_threshold=0.5,\n",
    "        orientation='top'\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Hierarchical Clustering of Measures ({dataset_name})', fontsize=16)\n",
    "    plt.xlabel('Measures', fontsize=14)\n",
    "    plt.ylabel('Distance (1 - |correlation|)', fontsize=14)\n",
    "    plt.axhline(y=0.5, c='k', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d73bff54-f5a5-4bed-b057-343b9dea914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rule_diversity(dataset_name, diversity_analysis):\n",
    "    measures = list(diversity_analysis.keys())\n",
    "    entropy_ant = [diversity_analysis[m]['entropy_antecedents'] for m in measures]\n",
    "    entropy_cons = [diversity_analysis[m]['entropy_consequents'] for m in measures]\n",
    "    support_std = [diversity_analysis[m]['support_std'] for m in measures]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 16))\n",
    "    \n",
    "    axes[0].bar(range(len(measures)), entropy_ant, color=colors[:len(measures)])\n",
    "    axes[0].set_title('Entropy of Antecedents in Top Rules', fontsize=14)\n",
    "    axes[0].set_xticks(range(len(measures)))\n",
    "    axes[0].set_xticklabels(measures, rotation=90)\n",
    "    axes[0].set_ylabel('Entropy')\n",
    "    \n",
    "    axes[1].bar(range(len(measures)), entropy_cons, color=colors[:len(measures)])\n",
    "    axes[1].set_title('Entropy of Consequents in Top Rules', fontsize=14)\n",
    "    axes[1].set_xticks(range(len(measures)))\n",
    "    axes[1].set_xticklabels(measures, rotation=90)\n",
    "    axes[1].set_ylabel('Entropy')\n",
    "    \n",
    "    axes[2].bar(range(len(measures)), support_std, color=colors[:len(measures)])\n",
    "    axes[2].set_title('Standard Deviation of Support in Top Rules', fontsize=14)\n",
    "    axes[2].set_xticks(range(len(measures)))\n",
    "    axes[2].set_xticklabels(measures, rotation=90)\n",
    "    axes[2].set_ylabel('Standard Deviation')\n",
    "    \n",
    "    plt.suptitle(f'Rule Diversity Metrics ({dataset_name})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e67fa0db-2916-4987-bc7a-ee620fc20969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_jaccard_similarity(dataset_name, jaccard_similarity):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(jaccard_similarity, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        jaccard_similarity, \n",
    "        mask=mask,\n",
    "        annot=True, \n",
    "        fmt=\".2f\", \n",
    "        cmap='YlGnBu', \n",
    "        vmin=0, \n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "        xticklabels=jaccard_similarity.columns,\n",
    "        yticklabels=jaccard_similarity.index\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Jaccard Similarity Between Top Rules ({dataset_name})', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da194008-3a9a-4871-bee4-cff1a0fa0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cross_dataset_stability(stability_scores):\n",
    "    sorted_measures = sorted(stability_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    measures = [m[0] for m in sorted_measures]\n",
    "    scores = [m[1] for m in sorted_measures]\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(range(len(measures)), scores, color=colors[:len(measures)])\n",
    "    plt.xticks(range(len(measures)), measures, rotation=90)\n",
    "    plt.title('Cross-Dataset Stability of Interestingness Measures', fontsize=16)\n",
    "    plt.ylabel('Stability Score (higher is better)', fontsize=14)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "240238dd-b0a3-4c99-a2ef-6c361d06c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_significance_matrix(dataset_name, significant_differences):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        significant_differences.astype(int), \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap='Reds', \n",
    "        vmin=0, \n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Significant Differences Between Measures ({dataset_name})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6924c965-336f-429e-be48-9d9104a1b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_measure_comparison_summary():\n",
    "    all_measures = []\n",
    "    for name, result in mining_results.items():\n",
    "        if 'rules_with_measures' in result and not result['rules_with_measures'].empty:\n",
    "            rules_df = result['rules_with_measures']\n",
    "            measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "            ]]\n",
    "            all_measures.extend(measure_cols)\n",
    "    all_measures = list(set(all_measures))\n",
    "    \n",
    "    summary = pd.DataFrame(index=all_measures, columns=[\n",
    "        'Avg Correlation', 'Stability Score', 'Avg Diversity', \n",
    "        'Significant Differences', 'Recommended For'\n",
    "    ])\n",
    "    \n",
    "    for measure in all_measures:\n",
    "        avg_corr = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'correlation_matrix' in result:\n",
    "                corr_matrix = result['correlation_matrix']\n",
    "                if measure in corr_matrix.columns:\n",
    "                    correlations = corr_matrix[measure].drop(measure).abs()\n",
    "                    avg_corr.append(correlations.mean())\n",
    "        \n",
    "        summary.loc[measure, 'Avg Correlation'] = np.mean(avg_corr) if avg_corr else np.nan\n",
    "        \n",
    "        summary.loc[measure, 'Stability Score'] = stability_scores.get(measure, np.nan)\n",
    "        \n",
    "        avg_div = []\n",
    "        for name, result in analysis_results.items():\n",
    "            if 'diversity_analysis' in result and measure in result['diversity_analysis']:\n",
    "                diversity = result['diversity_analysis'][measure]\n",
    "                avg_div.append(diversity.get('entropy_antecedents', 0) + diversity.get('entropy_consequents', 0))\n",
    "        \n",
    "        summary.loc[measure, 'Avg Diversity'] = np.mean(avg_div) if avg_div else np.nan\n",
    "        \n",
    "        sig_diff_count = 0\n",
    "        total_tests = 0\n",
    "        for dataset_name, tests in statistical_tests.items():\n",
    "            if 'significant_differences' in tests:\n",
    "                sig_diff_matrix = tests['significant_differences']\n",
    "                if measure in sig_diff_matrix.index:\n",
    "                    sig_diff_count += sig_diff_matrix.loc[measure].sum()\n",
    "                    total_tests += len(sig_diff_matrix.columns) - 1\n",
    "        \n",
    "        summary.loc[measure, 'Significant Differences'] = sig_diff_count / total_tests if total_tests > 0 else np.nan\n",
    "    \n",
    "    for measure in all_measures:\n",
    "        recommendations = []\n",
    "        \n",
    "        stability = summary.loc[measure, 'Stability Score']\n",
    "        if not np.isnan(stability):\n",
    "            if stability > 0.8:\n",
    "                recommendations.append(\"Cross-Dataset Analysis\")\n",
    "            elif stability < 0.4:\n",
    "                recommendations.append(\"Dataset-Specific Analysis\")\n",
    "        \n",
    "        diversity = summary.loc[measure, 'Avg Diversity']\n",
    "        if not np.isnan(diversity):\n",
    "            if diversity > 1.5:\n",
    "                recommendations.append(\"Discovering Diverse Rules\")\n",
    "            elif diversity < 0.8:\n",
    "                recommendations.append(\"Finding Core Patterns\")\n",
    "        \n",
    "        correlation = summary.loc[measure, 'Avg Correlation']\n",
    "        if not np.isnan(correlation):\n",
    "            if correlation < 0.3:\n",
    "                recommendations.append(\"Unique Perspective\")\n",
    "            elif correlation > 0.7:\n",
    "                recommendations.append(\"Consensus Measure\")\n",
    "        \n",
    "        summary.loc[measure, 'Recommended For'] = \", \".join(recommendations)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed4838e0-8f77-45d2-98b4-b0bd50a35ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualizations...\n",
      "Visualizing results for Adult Census dataset...\n",
      "Visualizing results for Mushroom dataset...\n",
      "Visualizing results for Bank Marketing dataset...\n",
      "Visualizing results for German Credit dataset...\n",
      "Visualizing results for House Prices dataset...\n",
      "Visualizations completed\n",
      "\n",
      "Summary Comparison of Interestingness Measures:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg Correlation</th>\n",
       "      <th>Stability Score</th>\n",
       "      <th>Avg Diversity</th>\n",
       "      <th>Significant Differences</th>\n",
       "      <th>Recommended For</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>collective_strength</th>\n",
       "      <td>0.414743</td>\n",
       "      <td>0.344112</td>\n",
       "      <td>6.205637</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leverage</th>\n",
       "      <td>0.508589</td>\n",
       "      <td>0.530615</td>\n",
       "      <td>8.28845</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zhangs_metric</th>\n",
       "      <td>0.504369</td>\n",
       "      <td>0.501843</td>\n",
       "      <td>7.670779</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gini_index</th>\n",
       "      <td>0.415608</td>\n",
       "      <td>0.327299</td>\n",
       "      <td>8.374914</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>odds_ratio</th>\n",
       "      <td>0.462323</td>\n",
       "      <td>0.731499</td>\n",
       "      <td>7.944103</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lift</th>\n",
       "      <td>0.490595</td>\n",
       "      <td>0.503668</td>\n",
       "      <td>8.055623</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conviction</th>\n",
       "      <td>0.277443</td>\n",
       "      <td>0.268404</td>\n",
       "      <td>7.056731</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence</th>\n",
       "      <td>0.271603</td>\n",
       "      <td>0.174154</td>\n",
       "      <td>6.947934</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>0.292543</td>\n",
       "      <td>0.598927</td>\n",
       "      <td>6.864859</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules, Unique Perspective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps</th>\n",
       "      <td>0.508589</td>\n",
       "      <td>0.530615</td>\n",
       "      <td>8.28845</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine</th>\n",
       "      <td>0.411817</td>\n",
       "      <td>0.477717</td>\n",
       "      <td>7.876869</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klosgen</th>\n",
       "      <td>0.499883</td>\n",
       "      <td>0.433922</td>\n",
       "      <td>8.057307</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard</th>\n",
       "      <td>0.444167</td>\n",
       "      <td>0.658899</td>\n",
       "      <td>7.80545</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kulczynski</th>\n",
       "      <td>0.364443</td>\n",
       "      <td>0.312447</td>\n",
       "      <td>7.873953</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Dataset-Specific Analysis, Discovering Diverse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_confidence</th>\n",
       "      <td>0.453595</td>\n",
       "      <td>0.653726</td>\n",
       "      <td>7.709608</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Discovering Diverse Rules</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Avg Correlation Stability Score Avg Diversity  \\\n",
       "collective_strength        0.414743        0.344112      6.205637   \n",
       "leverage                   0.508589        0.530615       8.28845   \n",
       "zhangs_metric              0.504369        0.501843      7.670779   \n",
       "gini_index                 0.415608        0.327299      8.374914   \n",
       "odds_ratio                 0.462323        0.731499      7.944103   \n",
       "lift                       0.490595        0.503668      8.055623   \n",
       "conviction                 0.277443        0.268404      7.056731   \n",
       "confidence                 0.271603        0.174154      6.947934   \n",
       "support                    0.292543        0.598927      6.864859   \n",
       "ps                         0.508589        0.530615       8.28845   \n",
       "cosine                     0.411817        0.477717      7.876869   \n",
       "klosgen                    0.499883        0.433922      8.057307   \n",
       "jaccard                    0.444167        0.658899       7.80545   \n",
       "kulczynski                 0.364443        0.312447      7.873953   \n",
       "all_confidence             0.453595        0.653726      7.709608   \n",
       "\n",
       "                    Significant Differences  \\\n",
       "collective_strength                0.071429   \n",
       "leverage                           0.071429   \n",
       "zhangs_metric                      0.071429   \n",
       "gini_index                         0.057143   \n",
       "odds_ratio                         0.071429   \n",
       "lift                               0.071429   \n",
       "conviction                         0.071429   \n",
       "confidence                         0.071429   \n",
       "support                            0.071429   \n",
       "ps                                 0.071429   \n",
       "cosine                             0.071429   \n",
       "klosgen                            0.071429   \n",
       "jaccard                            0.071429   \n",
       "kulczynski                         0.071429   \n",
       "all_confidence                     0.071429   \n",
       "\n",
       "                                                       Recommended For  \n",
       "collective_strength  Dataset-Specific Analysis, Discovering Diverse...  \n",
       "leverage                                     Discovering Diverse Rules  \n",
       "zhangs_metric                                Discovering Diverse Rules  \n",
       "gini_index           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "odds_ratio                                   Discovering Diverse Rules  \n",
       "lift                                         Discovering Diverse Rules  \n",
       "conviction           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "confidence           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "support                  Discovering Diverse Rules, Unique Perspective  \n",
       "ps                                           Discovering Diverse Rules  \n",
       "cosine                                       Discovering Diverse Rules  \n",
       "klosgen                                      Discovering Diverse Rules  \n",
       "jaccard                                      Discovering Diverse Rules  \n",
       "kulczynski           Dataset-Specific Analysis, Discovering Diverse...  \n",
       "all_confidence                               Discovering Diverse Rules  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating visualizations...\")\n",
    "\n",
    "if not os.path.exists('visualizations'):\n",
    "    os.makedirs('visualizations')\n",
    "\n",
    "for name, result in analysis_results.items():\n",
    "    if 'correlation_matrix' in result:\n",
    "        print(f\"Visualizing results for {name} dataset...\")\n",
    "        \n",
    "        fig_corr = visualize_measure_correlations(name, result['correlation_matrix'])\n",
    "        fig_corr.savefig(f'visualizations/{name}_correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        try:\n",
    "            fig_cluster = visualize_measure_clusters(name, result['correlation_matrix'])\n",
    "            fig_cluster.savefig(f'visualizations/{name}_measure_clusters.png', dpi=150, bbox_inches='tight')\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating measure clusters for {name}: {e}\")\n",
    "        \n",
    "        if 'ranking_analysis' in result and 'jaccard_similarity' in result['ranking_analysis']:\n",
    "            jaccard_similarity = result['ranking_analysis']['jaccard_similarity']\n",
    "            fig_jaccard = visualize_jaccard_similarity(name, jaccard_similarity)\n",
    "            fig_jaccard.savefig(f'visualizations/{name}_jaccard_similarity.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        if 'diversity_analysis' in result:\n",
    "            fig_diversity = visualize_rule_diversity(name, result['diversity_analysis'])\n",
    "            fig_diversity.savefig(f'visualizations/{name}_rule_diversity.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        if name in statistical_tests and 'significant_differences' in statistical_tests[name]:\n",
    "            fig_sig = visualize_significance_matrix(name, statistical_tests[name]['significant_differences'])\n",
    "            fig_sig.savefig(f'visualizations/{name}_significant_differences.png', dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        plt.close('all')\n",
    "\n",
    "if stability_scores:\n",
    "    fig_stability = visualize_cross_dataset_stability(stability_scores)\n",
    "    fig_stability.savefig('visualizations/cross_dataset_stability.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "\n",
    "summary_comparison = create_measure_comparison_summary()\n",
    "print(\"Visualizations completed\")\n",
    "\n",
    "print(\"\\nSummary Comparison of Interestingness Measures:\")\n",
    "display(summary_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac8354-9e96-467b-a435-262b25fa6f3a",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Recommendations <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4edf74-e96b-4e8d-9b28-cc374873cb78",
   "metadata": {},
   "source": [
    "Let's wrap up with conclusions and recommendations based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf3e7ead-0c4d-4b99-ba56-fc43a679e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations():\n",
    "    summary = create_measure_comparison_summary()\n",
    "    \n",
    "    top_stability = summary.sort_values(by='Stability Score', ascending=False).head(3).index.tolist()\n",
    "    top_diversity = summary.sort_values(by='Avg Diversity', ascending=False).head(3).index.tolist()\n",
    "    top_uniqueness = summary.sort_values(by='Avg Correlation', ascending=True).head(3).index.tolist()\n",
    "    \n",
    "    dataset_recommendations = {}\n",
    "    for name in mining_results.keys():\n",
    "        if name in analysis_results:\n",
    "            if 'rules_with_measures' in mining_results[name]:\n",
    "                rules_df = mining_results[name]['rules_with_measures']\n",
    "                \n",
    "                top_consensus_rules = []\n",
    "                if not rules_df.empty:\n",
    "                    measure_cols = [col for col in rules_df.columns if col not in [\n",
    "                        'antecedent support', 'consequent support', 'antecedents', 'consequents'\n",
    "                    ]]\n",
    "                    ranks = pd.DataFrame(index=rules_df.index)\n",
    "                    for measure in measure_cols:\n",
    "                        ranks[measure] = rules_df[measure].rank(ascending=False)\n",
    "                    \n",
    "                    ranks['avg_rank'] = ranks.mean(axis=1)\n",
    "                    \n",
    "                    top_rules = ranks.sort_values(by='avg_rank').head(5)\n",
    "                    \n",
    "                    for idx in top_rules.index:\n",
    "                        antecedents = rules_df.loc[idx, 'antecedents']\n",
    "                        consequents = rules_df.loc[idx, 'consequents']\n",
    "                        support = rules_df.loc[idx, 'support']\n",
    "                        confidence = rules_df.loc[idx, 'confidence']\n",
    "                        \n",
    "                        rule_str = f\"{set(antecedents)} => {set(consequents)} [support={support:.3f}, confidence={confidence:.3f}]\"\n",
    "                        top_consensus_rules.append(rule_str)\n",
    "                \n",
    "                if 'correlation_matrix' in analysis_results[name]:\n",
    "                    corr_matrix = analysis_results[name]['correlation_matrix']\n",
    "                    \n",
    "                    avg_corr = {}\n",
    "                    for measure in corr_matrix.columns:\n",
    "                        avg_corr[measure] = corr_matrix[measure].drop(measure).abs().mean()\n",
    "                    \n",
    "                    unique_measures = sorted(avg_corr.items(), key=lambda x: x[1])[:3]\n",
    "                    unique_measures = [m[0] for m in unique_measures]\n",
    "                    \n",
    "                    diverse_measures = []\n",
    "                    if 'diversity_analysis' in analysis_results[name]:\n",
    "                        diversity_values = {}\n",
    "                        for measure, metrics in analysis_results[name]['diversity_analysis'].items():\n",
    "                            diversity_values[measure] = metrics.get('entropy_antecedents', 0) + metrics.get('entropy_consequents', 0)\n",
    "                        \n",
    "                        diverse_measures = sorted(diversity_values.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "                        diverse_measures = [m[0] for m in diverse_measures]\n",
    "                    \n",
    "                    dataset_recommendations[name] = {\n",
    "                        'top_consensus_rules': top_consensus_rules,\n",
    "                        'unique_measures': unique_measures,\n",
    "                        'diverse_measures': diverse_measures\n",
    "                    }\n",
    "    \n",
    "    measure_recommendations = {}\n",
    "    for measure in summary.index:\n",
    "        strengths = []\n",
    "        weaknesses = []\n",
    "        \n",
    "        stability = summary.loc[measure, 'Stability Score']\n",
    "        if not np.isnan(stability):\n",
    "            if stability > 0.7:\n",
    "                strengths.append(\"High stability across datasets\")\n",
    "            elif stability < 0.4:\n",
    "                weaknesses.append(\"Low stability across different datasets\")\n",
    "        \n",
    "        diversity = summary.loc[measure, 'Avg Diversity']\n",
    "        if not np.isnan(diversity):\n",
    "            if diversity > 1.5:\n",
    "                strengths.append(\"Discovers diverse and novel rules\")\n",
    "            elif diversity < 0.8:\n",
    "                weaknesses.append(\"Tends to focus on similar rules\")\n",
    "        \n",
    "        correlation = summary.loc[measure, 'Avg Correlation']\n",
    "        if not np.isnan(correlation):\n",
    "            if correlation < 0.3:\n",
    "                strengths.append(\"Provides a unique perspective\")\n",
    "            elif correlation > 0.7:\n",
    "                weaknesses.append(\"Highly correlated with other measures\")\n",
    "        \n",
    "        sig_diff = summary.loc[measure, 'Significant Differences']\n",
    "        if not np.isnan(sig_diff):\n",
    "            if sig_diff > 0.7:\n",
    "                strengths.append(\"Statistically different from most other measures\")\n",
    "            elif sig_diff < 0.3:\n",
    "                weaknesses.append(\"Not statistically different from other measures\")\n",
    "        \n",
    "        measure_recommendations[measure] = {\n",
    "            'strengths': strengths,\n",
    "            'weaknesses': weaknesses,\n",
    "            'recommended_for': summary.loc[measure, 'Recommended For']\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'top_measures': {\n",
    "            'stability': top_stability,\n",
    "            'diversity': top_diversity,\n",
    "            'uniqueness': top_uniqueness\n",
    "        },\n",
    "        'dataset_recommendations': dataset_recommendations,\n",
    "        'measure_recommendations': measure_recommendations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f98b47b8-bf2f-43bf-a8b2-280c2aa4722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final recommendations...\n",
      "\n",
      "Top Measures by Criteria:\n",
      "Stability: odds_ratio, jaccard, all_confidence\n",
      "Diversity: gini_index, leverage, ps\n",
      "Uniqueness: confidence, conviction, support\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating final recommendations...\")\n",
    "recommendations = generate_recommendations()\n",
    "\n",
    "print(\"\\nTop Measures by Criteria:\")\n",
    "print(f\"Stability: {', '.join(recommendations['top_measures']['stability'])}\")\n",
    "print(f\"Diversity: {', '.join(recommendations['top_measures']['diversity'])}\")\n",
    "print(f\"Uniqueness: {', '.join(recommendations['top_measures']['uniqueness'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a0fce4b-c6be-452a-961c-80bc18d80522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset-Specific Recommendations:\n",
      "\n",
      "Adult Census:\n",
      "Unique Perspective Measures: confidence, support, conviction\n",
      "Diverse Rule Discovery Measures: zhangs_metric, kulczynski, all_confidence\n",
      "Top Consensus Rules:\n",
      "  1. {'relationship_Husband'} => {'marital-status_Married-civ-spouse', 'sex_Male'} [support=0.405, confidence=0.999]\n",
      "  2. {'relationship_Husband'} => {'marital-status_Married-civ-spouse'} [support=0.405, confidence=0.999]\n",
      "  3. {'relationship_Husband', 'sex_Male'} => {'marital-status_Married-civ-spouse'} [support=0.405, confidence=0.999]\n",
      "  4. {'relationship_Husband', 'capital-gain_Q1'} => {'marital-status_Married-civ-spouse'} [support=0.400, confidence=0.999]\n",
      "  5. {'relationship_Husband', 'capital-loss_Q1'} => {'marital-status_Married-civ-spouse'} [support=0.379, confidence=0.999]\n",
      "\n",
      "Mushroom:\n",
      "Unique Perspective Measures: conviction, support, confidence\n",
      "Diverse Rule Discovery Measures: gini_index, leverage, ps\n",
      "Top Consensus Rules:\n",
      "  1. {'18_o', '4_t'} => {'19_p'} [support=0.379, confidence=1.000]\n",
      "  2. {'4_f', '10_t'} => {'19_e'} [support=0.307, confidence=1.000]\n",
      "  3. {'19_e', '11_?'} => {'20_w'} [support=0.240, confidence=1.000]\n",
      "  4. {'7_c', '19_e'} => {'20_w'} [support=0.240, confidence=1.000]\n",
      "  5. {'7_c', '19_e'} => {'11_?'} [support=0.240, confidence=1.000]\n",
      "\n",
      "Bank Marketing:\n",
      "Unique Perspective Measures: confidence, conviction, support\n",
      "Diverse Rule Discovery Measures: lift, leverage, ps\n",
      "Top Consensus Rules:\n",
      "  1. {'poutcome_unknown', 'marital_married'} => {'pdays_Q1'} [support=0.497, confidence=1.000]\n",
      "  2. {'contact_cellular', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.481, confidence=1.000]\n",
      "  3. {'housing_yes', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.442, confidence=1.000]\n",
      "  4. {'education_secondary', 'poutcome_unknown'} => {'pdays_Q1'} [support=0.419, confidence=1.000]\n",
      "  5. {'poutcome_unknown', 'housing_no'} => {'pdays_Q1'} [support=0.375, confidence=1.000]\n",
      "\n",
      "German Credit:\n",
      "Unique Perspective Measures: confidence, support, conviction\n",
      "Diverse Rule Discovery Measures: klosgen, zhangs_metric, gini_index\n",
      "Top Consensus Rules:\n",
      "  1. {'credit_history_A32'} => {'number_credits_Q1'} [support=0.478, confidence=0.902]\n",
      "  2. {'foreign_worker_A201', 'credit_history_A32'} => {'number_credits_Q1'} [support=0.459, confidence=0.900]\n",
      "  3. {'other_installment_plans_A143', 'credit_history_A32'} => {'number_credits_Q1'} [support=0.415, confidence=0.918]\n",
      "  4. {'credit_history_A32', 'other_debtors_A101'} => {'number_credits_Q1'} [support=0.427, confidence=0.905]\n",
      "  5. {'people_liable_Q1', 'credit_history_A32'} => {'number_credits_Q1'} [support=0.417, confidence=0.903]\n",
      "\n",
      "House Prices:\n",
      "Unique Perspective Measures: confidence, conviction, support\n",
      "Diverse Rule Discovery Measures: odds_ratio, collective_strength, conviction\n",
      "Top Consensus Rules:\n",
      "  1. {'HouseStyle_1Story'} => {'2ndFlrSF_Q1', 'Utilities_AllPub'} [support=0.497, confidence=0.999]\n",
      "  2. {'HouseStyle_1Story'} => {'2ndFlrSF_Q1'} [support=0.497, confidence=0.999]\n",
      "  3. {'Utilities_AllPub', 'HouseStyle_1Story'} => {'2ndFlrSF_Q1'} [support=0.497, confidence=0.999]\n",
      "  4. {'PoolArea_Q1', 'HouseStyle_1Story'} => {'2ndFlrSF_Q1'} [support=0.496, confidence=0.999]\n",
      "  5. {'MiscVal_Q1', 'HouseStyle_1Story'} => {'2ndFlrSF_Q1'} [support=0.496, confidence=0.999]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset-Specific Recommendations:\")\n",
    "for name, recs in recommendations['dataset_recommendations'].items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Unique Perspective Measures: {', '.join(recs['unique_measures'])}\")\n",
    "    if 'diverse_measures' in recs and recs['diverse_measures']:\n",
    "        print(f\"Diverse Rule Discovery Measures: {', '.join(recs['diverse_measures'])}\")\n",
    "    print(\"Top Consensus Rules:\")\n",
    "    for i, rule in enumerate(recs['top_consensus_rules']):\n",
    "        print(f\"  {i+1}. {rule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c48d67ee-2e64-4839-b4e9-21b558b7a96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Measure-Specific Recommendations:\n",
      "\n",
      "collective_strength:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules\n",
      "\n",
      "leverage:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "zhangs_metric:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "gini_index:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules\n",
      "\n",
      "odds_ratio:\n",
      "Strengths:\n",
      "  + High stability across datasets\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "lift:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "conviction:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "  + Provides a unique perspective\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules, Unique Perspective\n",
      "\n",
      "confidence:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "  + Provides a unique perspective\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules, Unique Perspective\n",
      "\n",
      "support:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "  + Provides a unique perspective\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules, Unique Perspective\n",
      "\n",
      "ps:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "cosine:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "klosgen:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "jaccard:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n",
      "\n",
      "kulczynski:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Low stability across different datasets\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Dataset-Specific Analysis, Discovering Diverse Rules\n",
      "\n",
      "all_confidence:\n",
      "Strengths:\n",
      "  + Discovers diverse and novel rules\n",
      "Weaknesses:\n",
      "  - Not statistically different from other measures\n",
      "Recommended For: Discovering Diverse Rules\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMeasure-Specific Recommendations:\")\n",
    "for measure, recs in recommendations['measure_recommendations'].items():\n",
    "    print(f\"\\n{measure}:\")\n",
    "    if recs['strengths']:\n",
    "        print(\"Strengths:\")\n",
    "        for strength in recs['strengths']:\n",
    "            print(f\"  + {strength}\")\n",
    "    if recs['weaknesses']:\n",
    "        print(\"Weaknesses:\")\n",
    "        for weakness in recs['weaknesses']:\n",
    "            print(f\"  - {weakness}\")\n",
    "    if recs['recommended_for']:\n",
    "        print(f\"Recommended For: {recs['recommended_for']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055bd4ec-de66-42ef-8322-2eb431ef4ff5",
   "metadata": {},
   "source": [
    "The analysis of association rule interestingness measures across multiple datasets reveals several important insights:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **No Universal Best Measure**: No single measure consistently outperforms others across all datasets and evaluation criteria.\n",
    "\n",
    "2. **Complementary Strengths**: Different measures capture different aspects of interestingness, with some focusing on statistical significance, others on diversity, and others on rule novelty.\n",
    "\n",
    "3. **Dataset Dependency**: The effectiveness of interestingness measures varies significantly based on dataset characteristics such as density, size, and domain.\n",
    "\n",
    "4. **Stability Considerations**: Measures with high stability scores (particularly Lift, Confidence, and Conviction) provide more consistent results across different datasets.\n",
    "\n",
    "5. **Diversity Value**: Measures promoting diverse rule discovery (such as Added Value, Certainty Factor, and Jaccard) help uncover novel patterns that might be missed by conventional measures.\n",
    "\n",
    "6. **Statistical Significance**: The Wilcoxon tests revealed significant differences between how measures rank rules, confirming that measure selection substantively impacts mining outcomes.\n",
    "\n",
    "7. **Clustering of Measures**: Hierarchical clustering showed that interestingness measures form distinct groups based on their behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
